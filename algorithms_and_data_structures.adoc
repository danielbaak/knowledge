:encoding: UTF-8
// The markup language of this document is AsciiDoc

= Algorithms & data structures


== Asymptotic notations / Big O notation
In computer science, big O notation is used to classify algorithms by how they respond to changes in input size, typically regarding running time space (memory/disk/...).

In the following +n+ is the _input size_, +f(n)+ is the _number of steps_ needed by an algorithm, and  +T(n)+ is its _running time_.

Types of asymptotic notations:

[cols="3,2,2,6,6"]
|====
| notation | | relation of growth rate | definition | Notes
| +f(n) ∊ ο(g(n))+ | little-oh | f < g | Like O, bug for _all_ positive c | f is dominated by g asymptotically.
| +f(n) ∊ O(g(n))+ | big-oh    | f ≤ g | ++There exist an c>0 and n~0~>0 such that \|f(n)\| ≤ c⋅\|g(n)\| for all n≥n~0~++ | Asymptotic upper bound. No claim on how tight; technically it woudn't be wrong to say that a linear algorigthm is +O(2^n)+
| +f(n) ∊ Θ(g(n))+ | big-theta | f = g | ++There exist an c~1~>0, c~2~>0 and n~0~>0 such that c~1~⋅\|g(n)\| ≤ \|f(n)\| ≤ c~2~⋅\|g(n)\| for all n≥n~0~++ | Asymptotic tight bound. +Θ(g(n)) = O(g(n)) ∩ Ω(g(n))+. 
| +f(n) ∊ Ω(g(n))+ | big-omega | f ≥ g | Like O, but ≥ (instead ≤) | Asymptotic lower bound.
| +f(n) ∊ ω(g(n))+ | little-omega | f > g | Like ο, but ≥ (instead ≤) | f dominates g asymptotically.
|====

Θ is also called _rate/order of growth_.

Note: Because ++O(g(n))++ is really a set, we should actually write ++f(n) ∊ O(g(n))++.  However we often write ++f(n)=O(g(n))++, the equal sign meaning ∊.
Informally, especially in computer science, the big-oh notation often is permitted to be somewhat abused to describe an asymptotic tight bound (it really only describes an asymptotic upper bound) where using big-theta notation might be more factually appropriate in a given context.

_worst case_ / _average case_ / _best case_ refers to the worst / average / best input -- a ``good'' input results in a short running time of the algorithm, a ``bad'' input results in a long running time.  For many algorithms we only care about the worst case, not the average case, because a) the worst case occurs fairly often in practice b) the average case is often as bad as the worst case c) it's difficult to know what an ``average'' input is (often it is assumed that all possible inputs are equally likely).

_Tight bounds_: An upper bound is said to be a tight upper bound (aka _supremum_) if no smaller value is an upper bound.  Likewise for tight lower bounds (aka _infimum_).

_Asymptotic efficiency_: Only look at rate of growth.  Usually, an algorithm that is asymptotically more efficient will be the best choice for all but very small inputs.  An algorithm is said to be _asymptotically optimal_ if, roughly speaking, its big-oh is equal to the big-oh of the best possible algorithm.

_amortized time_: `amortized +O(f)+' for operation o: In a sequence of length L of such o operations, the overall time is +O(L*f)+.  I.e. one of those o operations might use a particular large amount of time compared to the average case, but that time is amortized in the large.  A typical example is appending to an array; if the capacity is full, a new array of larger capacity needs to be allocated, and the data has to be copied.

Common functions ordered after order of growth: c, log~c~(n), n, n·log~c~(n), n^c^, c^n^, n!, n^n^


See also:

- http://bigocheatsheet.com/
- http://stackoverflow.com/questions/1364444/difference-between-big-o-and-little-o-notation
- http://stackoverflow.com/questions/2986074/algorithm-analysis-orders-of-growth-question


== Computational complexity classes

The field of computational complexity categorizes decidable decision problems by how difficult they are to solve. "Difficult", in this sense, is described in terms of the needed computational resources.

A _decision problem_ is a problem with a binary answer, e.g. yes or no.  A _function problem_ can have answers that are more complex than a simple `yes' or `no'.  Function problems can be transformed into decision problems and vice versa.  Thus computational complexity can focus on decision problems. An _intractable_ problem is one that can be solved in theory (i.e. which is in R), but which in practice takes too long to be usefull. There's no exact definition, but in general problems not in P (but in R) are considered intractable.


P:: (Decision) problems solvable in at most polynomial tyme (n^c^).  If you can establish a problem as not in P, you provide good evidence for its intractability.  You'd better spend your time developing an approximation algorithm or solve a tractable special case.

NP (non-determiniatic polynomial):: (Decision) problems solveable in polynomial time via a ``lucky'' algorithm: Like in dynamic programm the algorithm makes a guess at each branch points where it could follow multiple paths.  However, if the overall answer of the decision problem is yes, it magically (being an awsome cool fairy tale computer) always guesses the path that ultimatively leads to the yes.
+
Equivalently: (Decision) problems where the a given yes-answer (e.g. yes, this sudoku has a solution), has a proof (can take more than polynomial time) (e.g. this solved sudoku) which can be checked in at most polynomial time (e.g. take the alleged solution / proof and verify it holds up to the sudoku rules).
+
Is a nondeterministic computation model.  It's not a realistic model, but it's still a usefull model.

EXP:: (Decision) problems solvable in at most exponential tyme (2^n^).

R (recursive):: (Decision) problems solvable in finite time. Etymology: R stands for recursive, which in the old days stood for `will terminate'.

NP-hard (or X-hard in general):: at least as hard as every element in NP (X in general) (i.e. same hardness or harder, but not less hard than any element in NP (X in general))

N-complete (or X-complete in general):: intersection of NP and NP-hard.

[[pseudo_polynomial]]
pseudo polynomial:: a numeric algorithm runs in pseudo-polynomial time if its running time is polynomial in the numeric value of the input, but is exponential in the length of the input – the number of bits required to represent it.  E.g. <<knapsack>>.

Set of problems, orderer on a line after hardness:

--------------------------------------------------
              P-complete  NP-complete  EXP-complete    R-complete
easier <----------|----------|-------------|---------|------> harder
      
P(incl P-complete)   P-hard (incl P-complete)
<---------------->|<---------------------------------....

      NP (incl NP-complete)     NP-hard (incl NP-complete)
<--------------------------->|<---------------------....

           EXP (incl EXP-complete)           EXP (incl EXP-complete)
<---------------------------------------->|<---------------------....
--------------------------------------------------

Most people think P≠NP is true, but no one could prove it so far (it's one of the Millenium Prize Problems), so it could be that P=NP is true.  P≠NP translates to ``you can't engineer luck'', or to  ``solving problems is harder than check solutions''.  NP is an awfully powerfull model of computation -- it can use this fairy tale computer which always magically guesses the right path -- so NP `obviously' is more powerfull than P -- except we don't know how to proof it.

Examples of NP-complete problems:

- Determining whether a graph contains a simple path with at least a given number of edges
- <<TSP,Travelling salesman problem>>
- <<knapsack>>
- _Hamiltonian path/cycle_: a path/cycle in an undirected or directed graph that visits each vertex exactly once
- _Boolean satisfiability_ (_SAT_) problem: *to-do*:
- _Subset sum problem_: Given a set (or multiset) of integers, is there a non-empty subset whose sum is zero?
- _clique problems_
 * Finding the maximum clique (a clique with the largest number of vertices)
 * Finding the maximum weight clique in a weighted graph
 * Listing all maximal cliques (cliques that cannot be enlarged)
- _minimum vertex cover_
- _maximum independent set problem_
- _Graph coloring_ regarding vertices (edges): Coloring the vertices (edges) of a graph such that no two adjacent vertices (edges) share the same color.


== Classifying Algorithms by Implementations

=== Recursion vs iteration

- What is computable by recursive functions is computable by an iterative model and vice versa.

- KISS: Use whichever is more easy to reason about for the given problem.  Since recursion maps easily to proof by induction, for many problems recursion is a straight forward choice.

* Recursion has to pay expense of function calls and function returns, which is typically larger than the (conditional) jump used in the iterative solution.  However in case of tail calls and an compiler featuring tail call optimization becomes pretty much equivalent to iteration since the machine code is iterative.

* Recursion needs memory on the stack for all the locals, the stack frame (the return address, the old stack pointer, ...).  However there are iterative solutions which need an stack or queue, which internally probably uses the heap with all its overhead in space and time.  It depends on the queue/stack implementation which is more efficient in terms of memory usage, locality, ....

- Modern compilers are good at converting some recursions to loops without even asking.


Terms: _base case_ is input for which the solution is directly known.  When the recursion arrives at the base case it is said to _bottom out_.

=== Recipes for convertion recursion -> iteration

==== Tail call
Recipe for translating recursion into iteration for a function ++foo++ for the case where recursive calls are convertible to tail calls:

. Convert all recursive calls into tail calls.  If you're programming language supports tail call optimization, you're already done.

. Enclose the body of the function with a ++while(true) { ... }++ loop.

. Replace each call to ++foo++ according to this scheme: ``++foo(f1(...), f2(...), ...)++'' => ``++x1=f1(...); x2=f2(...); ...; continue;++''

. For languages where identifiers need to be defined: For each +x+ object introduced in the previous step, define the object before the while loop introduced earlier.

. Tidy up.


==== Non tail call
`Recipe' for translating recursion into iteration in case there are n multiple recursive calls which are not tail calls and not convertible to tail calls.  It's more tips than a proper recipe.

- Remember that all local variables (which includes parameters) and the return address are on the stack.  So if one needs to know the return address, i.e. one of multiple possible places, it gets nasty difficult.

- Enclose the whole body in a ++stack<...> s; s.push(args); while (!s.empty()) { current_args = s.pop(); ... }++

- Instead of n times recursively calling foo like ++foo(args1); foo(args2);...++ push the args on the stack in reverse order ++s.push(args2); s.push(args1)++.




Recipe for turning a non-tail call recursive function ++foo++ into one having a tail call:

. Identify what work is being done between the recursive call and the return statement.  That delivers a function +g(x,y)+, so the respective expression could be written as ++return g(foo(...), bar)++.
. Extend the function to do that +g+ work for us.  Extend it with an new accumulator argument, ++foo(..., acc=default_doing_nothing)++, and replace all return statements ++return lorem;++ with ++return g(lorem, acc);++.
. Now you can replace very occurrence of ++return g(foo(...), bar)++ with ++return foo(..., bar)++, since we don't have to do +g+ ourselves any more, we can let +foo+ do +g+ for us.

--------------------------------------------------
// example step 1
def factorial(n):
    if n < 2: return 1
    return factorial(n - 1) * n // thus we have an g: g(x,y)=x*y

// example step 2
def factorial(n, acc=1):
     if n < 2: return 1 * acc
     return (n * factorial(n - 1)) * acc //==factorial(n-1)*(acc*n)

// example step 3
def factorial(n, acc=1):
     if n < 2: return acc * 1
     return factorial(n - 1, acc*n)
--------------------------------------------------
See also: http://blog.moertel.com/posts/2013-05-11-recursive-to-iterative.html


==== Non tail call

--------------------------------------------------
stack localsAndParamsStack;
stack addrStack;
addr = FunEntr;
auto done = false;
do {
  switch (addr) {
  case FunEntry:
    ...
  case X:
    ...
  }
} while (not done);
--------------------------------------------------


*to-do*: mind implicit return at end of original function

*to-do*: how to return values from called function?

How to translate calls and returns:

--------------------------------------------------
             function call                      | return
machine instr.     pseudo code in loop          | pseudo code in loop
 -----------------------------------------------|-------------------------
                                                | continue
                                                |
(save locals)      localsAndParamsStack.push(   | localsAndParams = 
                       locals and params)       |    localsAndParamsStack.pop()
                                                |
push params        params = new params          |
                                                |
push returnAddr    addrStack.push(addr)         |
                                                |
jmp funAddr        addr = FunEntry              | addr = addrStack.pop()
                   continue                     |
                                                |
                                                | if (addrStack.empty())
--------------------------------------------------


=== Deterministic vs non-deterministic
*to-do*

=== Serial vs parallel vs distributed
*to-do*

=== Exact vs approximate
*to-do*


== Algorithm design techniques/paradigms

=== Brute force (aka exhaustive search)
This is the naive method of trying every possible solution to see which is best.


[[divide_and_conquer]]
=== Divide and Conquer

_Divide_ the problem into two or more subproblems that are smaller instances of the same problem.  _Conquer_ the subproblems by solving them recursively.  If the size of a subproblem is small enough, stop recursion (we say the recursion _bottoms out_) and solve it (we call that small subproblem a _base case_) in a straightforward manner.  _Combine_ the solutions the subproblems into the solution of the original problem.  See also <<relation_between_techniques>>.

Examples: Quick sort


[[decrease_and_conquer]]
=== Decrease and conquer (aka prune and search)
In each step the problem is turned into one single sub problem of smaller size, where as the rest ist pruned.  The algorithm stops when the base case is reached.  My thoughts: The size of a subproblem is typically by a constant factor (on average) smaller than one of the parent problem -- if the size would only decrease by a constant amount, in the worst case 1, it would just be the naive brute force solution.  See also See also <<relation_between_techniques>>.

Examples: binary search, quickselect.


[[dynamic_programming]]
=== Dynamic programming (DP)
Basic idea: `carefull brute force'.  Use brute force, i.e. try (aka guess) all possible ways (and in case of optimization problems, take the best one).  However do that `carefully', by dividing the problem recursively into subproblems and use <<memoization>> to solve a particular subproblem only once.  Thus DP is often good for optimizations problems.  The memo is typically an associative array with +O(1)+ insert and lookup time.

Example problem referred to below: Consider a steel company cutting steel rods and selling the pieces.  For simplicity lengths are integers.  Given a table of prices which states the price for a rod of length i.  How to cut a rod of length n into multiple smaller rods to maximize revenue.

Dynamic programming needs two hallmarks:

1. _Optimal substructure_: An optimal solution to the problem contains within it optimal solutions to subproblems.  I.e. if you have an optimal solutions to each sub problem, you can combine them to form the optimal solution to the original problem.  Example: in the rod cutting problem, is optimally cutting a rod of length +n+ in two pieces.  That gives us two new subproblems: optimally cutting these two pieces.

2. _Overlapping subproblems_: A given sub-problem has to be solved/computed many times.  If that's not the case, there's no point in doing memoization.  Example: in the rod cutting problem, the problem of cutting a rod of length 2 has to be solved again and again within the problem of cutting a rod of length greater than 2.  Effectively the sub-problems form a directed graph, where x->y means subproblem x depends on subproblem y (i.e. y must be solved first).

DP in 6 steps:

1. _Define all subproblems_: I.e. define all vertices in the subproblem DAG. Details: Typically the input is a sequence of n items. For a given problem, it's subproblems are often either suffixes [i:] (Θ(n)) or prefixes [:i] (Θ(n)) or substrings [i:j] (Θ(n*n)).
2. _Guessing_: (I would say try-all): I.e. for each step (i.e. node / subproblem), think about all the possible paths (i.e. outgoing edges) I must try.
3. _Recurrence_: Same as step 2, but more formal: Formulate the recursive DP(...) function which returns the min/max/..., which includes defining the base cases.  Check that graph of subproblems is acyclic, i.e. is a DAG.
4. _Implement algorithm_: Implement DP(...), e.g. using one of the approaches presented below: top-down, bottom-up approach or shortest-path in DAG.
5. _Solve original problem_: Just call your algorithm with the right arguments.
6. _Reconstructing a solution_: Step 5 only gave a the value of the optimal solution (e.g. in case of the <<knapsack>> problem: the maximal value is 42), but you might also want to know which choices led there (e.g. which items to pack into the knapsack).
+
Variant 1) Each vertex also stores which choice it made.  Analogous to
DP(a,b,c,...), make it accessible e.g. via DPChoice(a,b,c,...).  Starting at
the root vertex, follow the path of those choices.
+
Variant 2) Starting at root of the DAG (e.g. DP(0,X) in the knapsack problem),
for current DP(a,b,c,...), try again, analogous to step 3, all possible paths
and take the one which results in the current DP(a,b,c,...), then recurse to
the choosen subproblem.

Approaches to implement the actual algorithm, see step 5 above:

_top-down approach_: DFS traverse the subproblem DAG from the root via recursion.  At each node, solve a particular problem only once (when it is first encountered) and in this case save its solution in the memo, and when it later is encountered again, look up the solution in the memo.

_bottom-up approach_: Iteratively solve the subproblems, in reverse topological order of the subproblem DAG.  Each iteration blindly uses the memo (knowing the solution must be there due to the topological order) and then memoizes the solution in memo. In general does the same computation as the top-down approach, provided you only solve those subproblems needed to ultimatively solve the orginal problem (e.g. a naive bottom-up approach of solving the _knapsack_ problem solves the whole DAG / matrix which includes nodes not reachable from the root / original problem).  Sometimes the bottom-up approach can save space, because you might know that you only need the last i solutions, e.g. in the fibonacci example you only need the last two. The topological sorted DAG helps to see if that is the case and how big i is.

_shortest path in DAG_: Often (*to-do*: when exactly / when not?) possible: Solve the <<shortest_path_problem>> (which is has a specialiced, more efficient version for DAGs) in the DAG.

Overall running time: +#subprobs * time/subprob+. #suprobs comes from step 1.  time/subprob comes from step 3; calculate running time of DP, ignoring the recursion.

Examples of those 5 steps, see <<edit_distance>>, <<knapsack>>.

--------------------------------------------------
# bottup-up                          # top down
                                     memo = {}
fun fib(n):                          fun fib(n):
  memo = {}                            if n in memo: return memo[n]
  for k=1 to(incl) n
    if k<=2: f = 1                     <--same
    else: f = memo[k-1]+memo[k-2]      <--" (recursive calls instead lookup)
    memo[k] = f                        <--"
                                       return f
--------------------------------------------------

Trivia: `Dynamic programming' is a wierd term, just take it for what it is. Still: in british english, `programming' means optimize.  The inventor, bellman, choose it for reasons among `sounds cool to a congress man', `to hide the fact he was doing math research'.

Example algorithms / example problems solvable with dynamic programing: Bellman-Ford, Floyd-Warshall, edit distance, knapsack (rod cutting problem, change-making problem), Dijkstra *to-do* more examples of problems which can be solved using dynamic programming, e.g. from the problems sections. https://en.wikipedia.org/wiki/Dynamic_programming


[[greedy_technique]]
=== Greedy technique / algorithm

A _greedy algorithm_ repeatedly makes locally best choice/decision, ignoring effect on future, with the hope, but not guarantee, of finding an optimal solution to the overall problem.

Problems for which a greedy algorithm works well generally have these two properties:

- _optimal substructure_: See also <<dynamic_programing>>.  Rational: The choice we just made (an optimal solution to a (mini) sub problem), plus the optimal solution to the subproblem that remains (which we will solve recursively), yiels an optimal solution to the original problem.

- _Greedy choice property_: Locally optimal choices lead to globally optimal solutions.

In many problems, a greedy strategy does not in general produce an optimal solution, but nonetheless a greedy heuristic may yield locally optimal solutions that approximate a global optimal solution in a reasonable time.  A greedy algorithm never reconsiders its choices; it makes locally best choices. This is the main difference from dynamic programming, which is exhaustive and is guaranteed to find the solution.

Example algorithms: (Greedy) best-first search, A*, Dijkstra, fractional knapsack problem, change-making problem for canonical coin system. *to-do*: more examples.


[[relation_between_techniques]]
=== Relation between techniques

Decrease and conquer is similar to divide and conquer.  However the latter splits the problem into two or more sub problems.  The former doesn't need to combine the results of the sub problems.

In dynamic programming, subproblems overlapp and we need to solve them only once. In divide/decrease and conquer, sub problems do not overlap.

dynamic programming vs greedy algorithm: in dynamic programming and divide/decrease and conquer the choices are made depending on the result of the sub problems. I.e. the sub problems are solved first.  The greedy algorithm makes first a (greedy) choice, thus reduces the problem to a subproblem, and then solves that remaining subproblem.


=== Linear programming
*todo*

=== Heuristic method
Such algorithms include local search, tabu search, simulated annealing, and genetic algorithms.

*to-do*

=== branch and bound

*to-do*


== Misc algorithm properies & terms

.Properties
Locality:: *to-do*
In-place:: An algorithm using +O(1)+ auxiliary memory space.  Often even +O(log n)+ is considered as in place.

.Terms
Sentinel:: A sentinel is a dummy object that allows us to simplify boundary conditions.
Online:: An online algorithm is one that can process its input piece-by-piece in a serial fashion, i.e. the algorithm does not need to see the entire input from the beginning.
Offline:: An offline algorithm requires to see the complete input from the beginning.
Memoization:: The solution to a given (sub)problem is memoized in a `memo pad' (aka table).  E.g. upfront or when first encountering it.  When later seeing the same (sub)problem again, its solution can be looked up in the memo.  See also <<dynamic_programming>>.
[[whp]]
With high probability (w.h.p.):: An event E occurs with high probability if Pr[E] ≥ 1−1/n^c^ for any constant c.

== Sorting
Properties of sorting algorithms.  See also properties of algorithms in general.  Comparison-based sorting algorithm are asymptotically optimal when they run in +O(n lg(n))+ time.

Stable:: Stable sorting algorithms maintain the relative order of records with equal keys
Adaptability:: Whether or not the presortedness of the input affects the running time.
internal/external sorting:: Internal: all the data fits into main memory. External: the input data does not fit into main memory, and parts of it must reside on secondary storage.

=== Overview

To sort arrays:

* Bubble sort, Insertion Sort and Selection Sort, having +O(n^2^)+, are bad. However insertion sort is online, stable, adaptive and has a small constant factor (also due to being CPU cache friendly), so it's often used for the base case of the recursive +O(n log n)+ algorithms. Bubble sort has tiny code size.
* Quick sort is great when it works, but unreliable (+O(n^2^)+ worst case), not stable, +O(log n)+ space for stack. Time complexity has a relatively small constant factor since it's CPU cache friendly.
* Merge sort is reliably good, stable, highly parallelizable, but requires +O(n)+ auxiliary space;
* Heap sort is reliably good, but unstable, and also about a factor of 4 slower than quick sort's best case.
* Introsort (hyprid of quick sort and heap sort) almost same as quicksort, now having the good +O(n log n)+ worst case, but constant factor is between quick sort and heap sort, i.e. worse than quick sort's average case.

To sort linked lists:

* Copy it to an temporary array, sort that, copy the array back to the linked list.  Main reason: array has much better locality than a linked list where the nodes are scattered within memory.
* Variant of merge sort

To sort strings:

* *to-do*

To sort integers:
* *to-do* see e.g. at 6:40 https://www.youtube.com/watch?v=pOKy3RZbSws&list=PLUl4u3cNGP61hsJNdULdudlRL493b-XZf&index=14


References:

- https://en.wikipedia.org/wiki/Sorting_algorithm#Comparison_of_algorithms
- Non comparison sorts with integers: http://en.wikipedia.org/wiki/Integer_sorting


=== Insertion sort / sort by insertion
Time: +O(n^2^)+ worst case & average case, +O(n)+ best case.  Auxiliary space: +O(1)+.  Adaptive.  Stable.  In-place.  Online.  Brief: In each outer iteration, the next element from the yet unsorted part is inserted into its correct position in the sorted part. More detailed: The input is logically divided into a sorted part at the left, initially empty, and an unsorted part at the right, initially the complete input.  In each outer iteration, insertion sort removes (see following swap) the leftmost element from the unsorted part.  In an inner iteration it drags the element to the location the elements belongs to within the sorted part by searching to the left and swapping elements on the way.  Often used for small arrays (since time complexity has a small constant factor).


=== Selection sort / sort by selection
Time: +O(n^2^)+ worst case & average case & best case.  Auxiliary space: +O(1)+.  Brief: In each outher iteration, select the min element from the yet unsorted part and append it to the sorted part. Detailed: Divide the input logically into a sorted part (initially empty) followed by an unsorted part (initially the whole input).  In each iteration search the smallest element in the unsorted part, swap it with the leftmost element of the unsorted part, then increment the pointer dividing the sorted / unsorted sub lists.


=== Bubble sort / sort by exchange
The input is devided logically into an unsorted part to the left, initially the whole input, followed by an sorted part, initially empty.  In each inner iteration, a sliding window of length two elements traverses the unsorted list from left to right, advancing in one element steps.  At each step, the two elements in the sliding window are swapped if needed to ensure the right element is larger than the left element.  The result of one inner iteration is that the sorted part gets one element added to its left side.  The process is repeated until all is sorted. As an optimisation, if an inner loop makes no swaps, it means the `unsorted' part is actually sorted and we can terminate early.


[[quick_sort]]
=== Quick sort
Time: +O(n^2^)+ worst case, +O(n lg(n))+ average case, best case: (simple partition: +O(n lg(n))+, 3way partition and equal keys: +O(n)+).  Auxillary space: worst case: (naive: +O(n)+, Sedgewick +O(log n)+).  Not stable in naive implementation.  Hidden factor in time complexity in practice quite small.  The +O(n^2^)+ worst case running time might be a problem when input size is large and used in an real-time system or system concerned with security (because malicious user potentially can trigger worst case behaviour).


=== (natural/external) Merge sort
Time: +O(n lg(n))+ worst case & average case & best case.  Space: +O(n)+ auxiliary memory.  Stable.  Good locality of reference.  Parallelizes well.  External sorting possible.  1) Divide the sequence into two equal length subsequences 2) sort the two sequences using recursion, recursion stops at a sequence of one element 3) merge the two sequences (see below).  Discussion: Good for sequentially accessible data.  Highly parallelizable (+O(log n)+).  Variant: _natural merge sort_:  Time: +O(n)+ best case, the rest remains equal.  Exploits any naturally occurring runs in the input.  Variant: _external merge sort_: Motivation: input data does not fit into memory.  Divide the input data into N blocks, each block fitting into memory.  Sort each block with any sorting algorithm and write the result to disk (N files).  Then with ``externally merge sorted sequences'' merge the blocks/files.  Variant _merge sort for linked lists_: *to-do*


=== Heap sort
Time: +O(n lg(n))+ worst case & average case & best case.  Auxillary Space: +O(1)+.  In-place.  Not stable.  In practice often somewhat slower that quick sort, however it has a better worst case run time.  Brief: As selection sort, however the min element is found via an binary min heap. Detailed: The input is logically divided into an unsorted part, initially the whole input, followed by a sorted part, initially empty.  The unsorted part is heapified into a max heap.  In each iteration, the first (i.e. max) element is swapped with the last,  thus appending a new element to the left side of the sorted part, and thus also shrinking the heap / unsorted part by one.  Using the heap's sift down operation (or just heapify again the unsorted part), the heap property is re-established.


=== Bucket sort
Time: +O(n^2^)+ worst case, +O(n+k)+ average-case and best case.  Auxiliary space: +O(n+k)+.  Stable.  Not comparison-based; it assumes that the elements' values are uniformly distributed, for simplicity, without loss of generality, assume in [0,1).  Make an array of k `buckets', where each bucket is a sequence of elements, initially empty.  For each element in the input, insert it into the bucket having the array index k*elementvalue.  Then sort each of the buckets with another sort.  Then produce the final output by concatenating the buckets.  Note: the time complexity gets worse if the data is not uniformly distributed as assumed, since certain bucket sequences get much longer than other.


=== Counting sort (aka histogram sort)
+Θ(k+n)+ time complexity, +Θ(k+n)+ or +Θ(k)+ space complexity, depending on whether the _required_ output array (sorting in-place only with the input array is not possible) is taken into account or not.  Not comparison-based; it assumes that each element is an integer in the range +0--k+, where +k=O(n)+ (If you start off with non-integers, you might be able to map to integers).  Stable sort.  Algorithm: With an array a of size k, for each input element x, increment a[x]. Then for each item (index i) in the array, print the number i a[i] times. 

--------------------------------------------------
k=5, input={3,4,2,2,4}
             1s   2s  3s  4s  5s
a           { 0,  2,  1,  2,  0 }
output      {     2,2,3,  4,4   }
--------------------------------------------------


=== Radix sort
Not comparison-based.  Stable sort.  Given n d-digit numbers/strings, each digit can take up to k possible values.  Time: +Θ(d(n+k))+ (provided the internally used stable sort has +Θ(n+k)+), however the hidden constant factor is in practice quite large relative to other sort algorithms.  First sort/group after least significant digit, then after second least, .... The inner sort must be a stable sort, typically counting sort or bucket sort.


=== Merge sorted sequences
Time: +Θ(n)+.  Space: +O(n)+. Imagine n cards within m sorted piles of cards face up.  Take the smallest card yous see and put it on the sorted pile.


=== Externally merge sorted sequences
Given N sorted sequences on disk which do not fit all together into memory.

For each, make a buffered (in memory) input stream.  Make an (buffered) output stream for the result.  Now there are N input streams and 1 output stream and the algorithm works as described in ``merge sorted sequences''.  You might want the I/O to be in separate threads, so the actual algorithm can run while there is IO filling parts of in input stream with new data, or flushing part of the output stream. However, if there are only B blocks/pages of memory available but there are more than B-1 input streams, multiple passes have to be made.


[[BST_sort]]
=== BST sort
Take an implementation of the BST ADT, insert all elements of the input, then do an in-order walk.

When the <<naive_BST>> is used, BST sort does same comparisons as <<quick_sort>>, but in a different order. (where quick sort uses first element as pivot, and then does stable partitioning). When the input is randomly permuted before building the BST, _randomized BST sort_ does again the same comparisons as randomized quick sort choosing its pivot randomly.

=== Partial sorting

Sort the k smallest elements.  Opposed to all elements, as in total sorting.

_partial heapsort_: Heapify to a min heap, then do m extractions.

_quickselsort_: Use quickselect to find k-th smallest element. The way that
algorithm works will leave it at the k-th position. Sort the elements [0,k).

|=====
|                    | time av.       
| partial heapsort   | O(n + k log n)
| quickselsort       | O(n + k log k)
|=====


[[order_statistics]]
[[Selection]]
== Order statistics / Medians / selection problem

_Selection problem_: Find the i-th order statistic, i.e. find the i-th smallest element out of an, typically unsorted, set of elements.

The _i-th order statistic_ is the i-th smallest element out of a set of elements.  For example, the _minimum_ of a set of elements is the first order statistic, and the _maximum_ is the n-th order statistic.  A _median_, informally, is the “halfway point” of the set.  When n is odd, the median is unique, occurring at i D .n C 1/=2.  When n is even, there are two medians, occurring at i D n=2 and i D n=2C1.  Thus, regardless of the parity of n, medians occur at i D b.n C 1/=2c (the _lower median_) and i D d.n C 1/=2e (the _upper median_).  For simplicity in this text, however, we consistently use the phrase “the median” to refer to the lower median.

Overview:

|======
|                                  | time av.               | time worst  
|via sort                          | O(sort(n))             | O(sort(n))
|quick select / random             | O(n) almost certain    | O(n^2)
|quick select / median of 3        | O(n)                   | O(n^2)
|quick select / median of medians  | O(n) high const factor | O(n) high const factor
|Introselect                       | O(n)                   | O(n)
|Floyd–Rivest algorithm            | O(n)                   | ? *to-do*
|======

=== Trivial algorithms
Finding the minimum, i.e. i=1, element can be trivially solved in +O(n)+ time and +O(1)+ auxillary space by iteratively searching through the array for the smallest element respectively.

If the input is preprocessed sorting it first, then we can just access the i-th element in the sorted collection in +O(1)+. Recalling sorting algorithms, the cost of the preprocessing is typically +O(n log n)+, but can be +O(n)+ for certain input. Obviously this might be a good strategy if the collection is static and thus has to be preprocessed only once, and there are more than +O(log n)+ queries (assuming +O(n log n)+ sorting and +O(n)+ for the competing selection algorithm).

=== Quick select

A <<decrease_and_conquer>> algorithm: In each recursion step, choose a pivot and partition the input (of the current recursion step) into a part smaller than the pivot and larger than the pivot respectively.  That delivers the position / index of the pivot.  Then recurse into the left/right part depending on whether i is smaller/larger than the pivot's index.  The base case is either if i is equal the pivot after partitioning, or when the input size is 1.

The best possible pivot is the median (apart from the i-th element, which would directly deliver the solution), since it halves the input.  Like quicksort, the quickselect has good average performance, but is sensitive to the pivot that is chosen.  See the next chapters on strategies how to choose the pivot.

==== Quick select / variant random pivot random
See the previous `common core' chapter. The pivot is chosen randomly.


==== Quick select / variant median of 3
The pivot is the median of the first, middle and last element. Also sort these three elements.


[[median_of_medians]]
==== Quick select / variant median of medians (aka BFPRT)
See the previous `quick select core' chapter. The chosen pivot is an approximate median (remember that the real median would be optimal). It is guaranteed being between 30th and 70th percentiles.

Algorithm to find pivot in array A: Make groups of five elements and find median in each of those (e.g. via insertion sort and taking 3rd element). Then recursively find median of those medians.

Trivia: BFPRT stands for the name of its inventors Blum-Floyd-Pratt-Rivest-Tarjan

*to-do*: Time, auxillary space?

=== Introselect
The idea is to start out with an decrease and conquer selection algorithm that has very good average performance but bad worst case performance, and if it on the fly remarks that it is making bad progress (i.e. steers towards the worst case), switch to choosing a selection algorithm that has optimal worst case perforamnce.

Concretely, starts out with the quick select variant which uses a random pivot, and potentially switches to the median of medians quick select variant.

Examples of possible switching strategies:

- If the sum of partitions so far exceeds the original input size times a constant factor. Only one variable needed to track sum of partitions so far.
- If at any point the last k partitions did not half the input size, where k is some small positive constant. 

Trivia: Introselect is short for introspective selection.

=== Order statistics tree

1) Augment a binary search tree by augmenting each node with the size of its subtree.

Select(i): O(log n) [however that does not include the cost of creating the tree!!]

2) Via min max heap

*to-do*

=== Floyd–Rivest
Functionally equivalent to quickselect, but runs faster in practice on average.

*to-do*


[[string_searching]]
== String searching / matching
Problem: Find all/some occurences of a pattern P in a given text T.  Let _Σ_ be an alphabet (finite set), _T_ a string of length _n_, _P_ a string of length _m_.  Both the pattern and searched text are vectors of elements of Σ.

_naive string search_: iteratively check at each location in the searched-text.  Time: +O((n-m+1)m)+ worst case, +O(n)+ average case (note that m<=n).

Comparision *to-do*: Small/big T, small/big P, small/big Σ, repeatedly searching in same T, repeatedly searching same P, ...
- http://programmers.stackexchange.com/questions/183725/which-string-search-algorithm-is-actually-the-fastest
-


=== Rabin-Karp
Time: +Θ(m)+ pre-processing, +Θ((n-m+1)m)+ worst case running time,  +O(n+m)+ expected running time.  *to-do*: I don't see why the naive approach should have a worse expected running time, or a worse constant factor if equal

Compute a hash of the pattern.  Iteratively move a window over the search text until the left edge of the window hits the end of the search text.  The window has the same length as the pattern.  In each iteration compute a rolling hash of the window.  If the window-hash matches the pattern-hash, do a regular string comparison between the window and the pattern, and if they still match, the pattern is found.

Popular rolling hash functions for Rabin-Karp:

--------------------------------------------------
static const int q = ...; // a prime where q*radix<INT_MAX
static const int h = pow(d, m-1) % q;

int find(const string& text, const string& pattern) {
   int radix = ...; // aka d.  size of alphabet, e.g. 127 or 255
   int textlen = text.length(); // aka n
   int patternlen = pattern.length(); // aka m
   int patternhash = hash(pattern, m); // aka p
   int texthash = hash(text, m); // aka ts
   for ( int s=0; s<=textlen-patternlen; ++s ) {
     if (patternhash==texthash && text.issubstring(s,pattern))
       return s;
     if (s<textlen-patternlen)
       texthash = rollinghash(texthash, text[s+1], text[s+patternlen+1]);
   }
   return -1;
}

int hash(const string& str, int len) {
    int acc = 0;
    for ( int i=0; i<len; ++i ) acc = (radix * acc + str[i]) % q;
    return acc;
}

int rollinghash(int hash, char ch_out, char ch_in) {
    return (radix*(hash - ch_out*h) + ch_in) % q;
}
--------------------------------------------------


=== Knuth-Morris-Pratt
Time O(n+m) worst case [O(m) preprocessing], O(m) auxillary space

*to-do*


=== Boyer-Moore
O(n+m) worst case

*to-do*


=== Misc

- <<suffix_tree>>
- _FSA_ / _DFA_: *to-do*


== Graph (incl. tree) Algorithms

See also <<graph>>.

[[BFS]]
=== Breadth-first search (BFS) / traversal

An algorithm for traversing or searching a graph in breadth first order in +O(V+E)+ (= +O(b^d+1^)+) time and +O(V)+ space.  Typically used to traverse a connected graph starting from a single source, thus that's what is shown here. Traversing a disconnected graphs, which implies multiple sources, is theoretically also possible, but not shown here. See the outer skeleton of DFS for the general idea how that would be done.

Intuitively the algorithm is: Starting at the source vertex (distance 0 (here distance counts number of edges)), first nodes with distance 1 (to the root) are explored, after that nodes with distance 2, and so on.

Overview version:
--------------------------------------------------
Breadth-First-Search(Graph, source:vertex):
  create empty queue Q
  Q.enqueue(root)
  while Q is not empty:
    current = Q.dequeue()
    for each neighbor node that is adjacent to current:
      if neighbor was never visited so far:
        remember neighbor as visited
        Q.enqueue(neighbor)
--------------------------------------------------

Time complexity +O(V + E)+ (each vertex is enqued/dequed at most once, and each edge is looked at twice (from each of its two sides)). Auxillary space complexity is +O(V)+ (Each vertex needs an color attribute. Beside that, in the worst case, the queue contains all vertices).  For graphs which are implicitly defined or very large, time complexity is better given as +(b^d+1^)+, whereas b is the branching factor and d is the distance (weights being 1) up to which we search.

To be able to know whether a given node (aka vertex) was already visited typically each node gets a `color' attribute and/or a distance attribute attached.  If it is known that the graph is acyclic that color attribute is not needed since it's inherently not possible for that algorithm to visit a node twice.

Detailed version:
--------------------------------------------------
Breadth-First-Search(graph, source:vertex [, dest:vertex]):
  init(graph, source, Q)
  while Q is not empty:
    current = Q.dequeue()
    [do auxillary visit action with current]

    for each neighbor that is adjacent to current:
      if neighbor.color == unvisited |or| neighbor.distance == NIL:
        neighbor.color = visited/exploring |and/or| neighbor.distance = current.distance + 1
        [neighbor.parent = current]
        [if source==dest: return] // if its only about finding path source->dest
        Q.enqueue(neighbor)

    [current.color = finished] // not needed in the two-colors schemes

init(graph, source:vertex):
  for each vertex v in graph:
    v.color = undiscovered |and/or| v.distance = NIL
    [v.parent = NIL]

  source.distance = 0 |and/or| source.color = tentative
  create empty queue Q
  Q.enqueue(source)
--------------------------------------------------

Applications:

- Solves the single-source <<shortest_path_problem>> where all edge weights are equal / absent, i.e. where for all paths the path weight equals the path distance.




There are multiple common naming schemes for the colors. The three color schemes have no advantage over the two color schemes other than some people find the algorithm easier to understand / visualize.

[options="header"]
|=====
   |scheme 1     | scheme 2     | scheme 3     | relation to distance
   |undiscovered | unvisited    | white        | NIL / infinite
.2+|discovered   | tentative    | gray      .2+| not NIL / not infinite
                 | visited      | black
|=====


The 3 colors split color 2 into two new separate colors 2.1 and 2.2.

unvisited/undiscovered
visited/discovered
exploring / tentative☹ | explored/finished

I propose: unvisited,visited(exploring, finished)

DFS does not really produce a spanning tree but a spanning forest. The book says `predecessor subgraph'.




==== Tree level order traversal: Recursive approach


==== Tree level order traversal: Iterative approach

Is basically a BFS, simplified by the fact that a tree is an acyclic graph, and that typically the distance is not something that we want to know, and thus neither the color nor the distance attribute is needed.

--------------------------------------------------
Tree-Level-Order-Traversal(root):
  create empty queue Q
  Q.enqueue(root)

  while Q is not empty:
    current = Q.dequeue()
    for each child of current:
      Q.enqueue(n)
--------------------------------------------------


[[DFS]]
=== Depth-first search (DFS) / traversal

An algorithm for traversing or searching a graph in depth first order.  In +Θ(V+E)+ time and +O(V)+ space.  Typically used to traverse the complete, possibly disconnected graph.  As opposed to search only a connected graph starting from a single source; however that is also possible.


==== Recursive algorithm

Basic algorithm for a connected graph:
--------------------------------------------------
DFS-visit(graph, source:vertex):
  mark source as visited
  for each neighbor of source:
    if neighbor is unvisited:
      DFS-visit(graph, source)
--------------------------------------------------

More detailed algorithm for a general graph:
[[DFS_all_source]]
--------------------------------------------------
DFS-all-source(graph):
  for each vertex in graph, set color=unvisited and parent=NIL
  for each vertex v in graph:
    if v.color == univisted:
      DFS-visit(graph, v)

DFS-single-source(graph, source:vertex):
  for each vertex in graph, set color=unvisited and parent=NIL
  DFS-visit(graph, source)

DFS-visit(graph, current:vertex):
  current.color = tentative
  [do auxillary pre-order action with current]
  for each neighbor adjacent to current: // aka explore edges (current,neighbor)
    if neighbor.color is unvisited:
      [neighbor.parent = current]
      DFS-visit(graph, neighbor)
  [do auxillary post-order action with current]
  [current.color = visited]              // not needed in the two color variants
  [push to front of topo sorted sequence]
--------------------------------------------------

Analysis DFS-all-source: Time complexity: +Θ(V+E)+. Rational: Each vertex is visited (is current) once -> +O(V)+.  Each outgoing edge of each current (i.e. each node) is looked at -> +O(E)+. Space complexity: +O(V)+. Rational: Each vertex has at least the color attribute attached. Also in the worst case, when the graph is a list, there are as many recursion calls (stack pushes) as vertices.


==== Iterative algorithm

Basic algorithm 1 for a connected graph:
--------------------------------------------------
DFS-visit(G:graph, root:vertex):
  while stack is not empty:
    current = stack.pop()
    if current.color == unvisited:
      current.color = visited
      for each neighbor of current:
        stack.push(neighbor)
--------------------------------------------------

Basic algorithm 2, which is closer to the detailed that follows, for a connected graph:
--------------------------------------------------
DFS-visit(G:graph, root:vertex):
  while stack is not empty:
    current = stack.top()
    if current.color == unvisited:
      current.color = visited
      for each neighbor of current:
        stack.push(neighbor)
    else
      stack.pop() 
--------------------------------------------------

The recursive DFS saves on its call stack for each recursion the pair 1) the current vertex and 2) for that current vertex the current neighbor in the sequence of neighbors it is iterating over. The following iterative solution saves neighbors on the stack, but not the `current neighbor'. E.g. when starting, _all_ neighbors of the src node are pushed immediately onto the stack, and only after that the next iteration begins; that is unlike the recursive solution, which only explores the 2nd neighbor of the src after the complete DFS tree of the first neighbor has been explored. 

Note that DFS-visit is only part of DFS-all-src, *to-do* make it correct where to place init

--------------------------------------------------
DFS-visit(G:graph, root:vertex):
  init(G, root, S)
  while S is not empty:
    current = S.top() // not pop
    if current.color = undiscovered
      [do auxillary pre-order action with current]
      current.color = tentative !!! bad wording. we definitely added to the DFS, but we're not yet finished with this vertex
      for each neighbor of current:
        if neighbor.color = undiscovered: <3>
          [neighbor.parent = current] <4>
          S.push(neighbor) <4> !!! tentatively adding to DFS does _not_ change color 
    elif current.color = tentative
      [do auxillary post-order action with current]
      current.color = discovered <2>
      [push to front of topo sorted sequence]
      S.pop()
    elif current.color = discovered <1>
      S.pop()

init(G:graph, root:vertex, S:stack)
  for each vertex v in G:
    v.color = undiscovered
    [v.parent = NIL]
  create empty stack S
   S.push(source)
--------------------------------------------------

<1> Skipping vertices that are already in the DFS tree. There is no direct equivalent in the recursive DFS version because that
<2> (Definitly) adding current vertex to the DFS tree
<3> `If guard' is only required when parent pointers needs to be set. The guard prevents from modifying the now immutable parent pointer of a vertex which already is in the DFS tree.  If the parent pointer is not needed, keeping the if nevertheless may or may not be an optimization: you save `needlessly' adding vertices to the stack (they will be skipped by ① anyway), but pay with one more conditional branch.
<4> Tentatively add neighbor to DFS tree

When there are multiple paths to a node, a node can occur multiple times in
the stack. Since we want depth-first, we want to follow the longer path (take
the vertex instance that is higher up in the stack), and discard the shorter path (the
instance further down in the stack, discarded by the continue case ①). E.g. in
the following graph, A being the root, DFS pushes A as part of the init, pops
A and marks it as discovered, pushes C and B onto the stack, pops B and marks
is as discovered, pushed C _again_, pops C and marks it as discovered, pops C
(the original push) again and skips it since its marked as discovered.

--------------------------------------------------
        A
      /   \
     V     V
     B---->C
--------------------------------------------------

*to-do*: symmetry between DFS and BFS: http://stackoverflow.com/questions/5278580/non-recursive-depth-first-search-algorithm


[[DFS_forest]]
[[DFS_edge_classification]]
==== Edge classification / DFS forest

These edge properties are edge properties of the spanning forest (aka DFS forest) that results from a particular DFS run, not of the graph per se.

The trees in the DFS forest produced by the algorithm above are in-trees (edges point towards root), i.e. edge direction is reversed relativ to the underlying graph.  When in the following ancestry terms are used, they refer to the edges in the DFS tree, not the edges in the underlying graph.

--------------------------------------------------
        A----->B  directed edge in graph
         <·····   directed edge in DFS forest
   parent      child  
  current      neighbor
--------------------------------------------------

_Tree edges_: node->child / neighbor is univisited (aka white). I.e. an edge in the DFS forest.

_Back edges_: node->anchestor / neighbor is exploring (aka gray). A self-loop is considered a back edge.

_Forward edges_: node->descendant(non-child) / neighbor is finished (aka black) and node.starttime<neighbor.starttime.

_Cross edges_: node->neither-anchestor-nor-descendant / neighbor is finished (aka black) and neighbor.starttime<node.starttime. Between two non-ancestor-related sub-trees (i.e. between trees in the depth-first forest or between sub-trees within a given tree).

--------------------------------------------------
      ---A<--
     /  / \  \
     | V   V  |
for  | B   C  |back edge
ward | |   |  |
edge | V   V  |
     \>D<--E-/
       cross edge
--------------------------------------------------

Applications:

- <<cycle_dedection>>: A graph is acyclic iff there are no back edges.


==== Tree (pre-/in-/post-) order traversal: Recursive approach

Trivial

==== Tree (pre-/in-/post-) order traversal: Iterative approach

Is a DFS, but since a tree has no cycles, the color attribute is not needed.

--------------------------------------------------
void traverse(Node* n) {
  MyStack<Node*> parents; // pop returns top element and removes it
  Node* prev = NULL; // prev is always non-NULL except at the beginning
  parents.push(prev);
  for ( Node* next = NULL; n ; prev = n, n = next) {
    // came from top
    if ( prev==parents.top() ) {
      preorder_visit(n);
      if (n->left) { // go down left
        parents.push(n);
        next = n->left;
      else if (n->right) { // skip left, go down right
        inorder_visit(n);
        parents.push(n);
        next = n->left;
      } else { // skip left, skip right, go up
        inorder_visit(n);
        postorder_visit(n);
        next = parents.pop();
      }
    }

    // came from left
    else if (prev == n->left) {
      inorder_visit(n);
      if (n->right) { // go right
        next = n->right;
      } else { // skip right, go up
        postorder_visit(n);
        next = parents.pop();
      }
    }

    // came from right
    else {
      postorder_visit(n);
      next = parents.pop(); // go up
    }
  }
}
--------------------------------------------------


=== (Greedy) best-first search
*to-do*:


[[A_star]]
=== A*
A* is a best-first-search algorithm which solves the single-pair <<shortest_path_problem>>, not allowing negative weights.  A* uses an heuristic, involving the function h, to find the next node to be added to the shortest path tree.  It can also be used to solve the single-source shortest-path problem, but is not intended for it, since h is typically optimized for a specific destination node.  A* is a generalized version of the Dijkstra's algorithm: if h returns always 0 (i.e. is independent of a destination node) then A* equals Dijkstra.

How the algorithm works intuitively (use case `monotonic h'):  Construct the shortest path tree (_SPT_) by adding source to the SPT, and then iteratively adding a vertex to the SPT.  The vertex v to be added to the SPT is the one with minimal `estimated shortest path length from source to destination via v'. That is correct due to the monotonic h and the fact that any sub path of a shortest path is itself a shortest path.

Each vertex gets attached two additional attributes: shortest path tree parent (*+spt_parent+*) and shortest path weight from source to this node (*+spwfs+*).  While a node is in the queue, these are tentative values. Once a node is dequed and thus really added to the shortest path tree, they remain at that final value.  In case of +spwfs+ its value is an upper bound on the true value.

As parameter the algorithm takes an heuristic function *h*, or more verbosely, +*estimated_spw_to_dest*(from:vertex)+, which shall return an estimated shortest path weight from the given vertex to the implicit destination vertex. More on h later.

From h another function is derived: ++function vertex::**k**() = .spwfs + estimated_spw_to_dest(this);++.  It returns the estimated shortest path weight from the implicit source vertex to the implicit destination vertex via vertex v.  k() is used as the key for the min priority queue.

Algorithm which assumes h is monotonic:

When dest==NIL the algorithm solves the single-source shortest path problem.  The solution is the shortest path tree given by the .spt_parent attribute of each vertex, along with the .spwfs attribute of each vertex.

When dest!=NIL the algorithm solves the single-pair shortest path problem.  The solution is the shortest path from source to dest with a weight of dest.spwfs and given by a linked list from dest to source, starting at dest.spt_parent.

--------------------------------------------------------------------------------
function A*(graph, source:&vertex, dest:&vertex,
        estimated_spw_to_dest:function(:vertex)->double )
    create min priority queue outsideSptQ  where key is vertex::k() <1>
    for all vertices v in graph:
        [v.spt_parent = NIL]
        v.spwfs       = (v==source ? 0 : INFINITE)
        outsideSptQ.enqueue(v)

    while outsideSptQ not empty:
        current = outsideSptQ.deque() <2>
        [if current == dest return]
        for each neighbor of current:
            relax(current, neighbor, outsideSptQ) <3>

function relax(u:vertex, v:&vertex, Q:MinPriorityQ)
    alternate_spwfs_for_v = u.spwfs + weight(u,v)
    if alternate_spwfs_for_v < v.spwfs:
        Q.decreaseKey(v, alternate_spwfs_for_v)
        [v.spt_parent = u]
--------------------------------------------------------------------------------

Optimizations and notes to the above algorithm:

<1> Optionally adapt the min priority queue implementation slightly: If +dequeue+ returns NIL if all remaining keys are infinite, we can sooner terminate.  Also the implementation might take advantage of the knowledge that many keys are infinite, especially at the beginning.
<2> Greedely choose vertex with smallest k and add it to the SPT. +current.spwfs+ is now at its final true value, i.e. no longer an estimate / upper bound.
<3> Note that +neighbor+ could already be part of shortest path tree and thus could be skipped.  However it's not worth explicitly checking that (in the presented implementation there would also not be a trivial way to do so), since the if statement within +relax+ implicitely ensures that not much is made with +v+.

Properties of the A* algorithm:

- complete (will always find a path if one exists)
- optimal (*to-do*: i think this explains optimal wrongly: finds the shortest path), but only if the provided heuristic function is admissible.  However see <<bounded_relaxation>> later in the A* sub chapter.
- *to-do:* I don't trust the time complexity, space complexity noted in Wikipedia,  I guess they forget to account for the costs of the min priority queue and of h.  If h is const, then there two options: call it once for every vertex at the beginning and store the result, or call it every time when needed.  Depending on the problem one or the other will be more optimal.
- *to-do:* Does it work for the case when the single-source shortest path problem is tried to be solved?
- Is an best-first search, but not greedy

About the heuristic function h aka +estimated_spw_to_dest(from:vertex)+:

- Responsibility: Returns an estimated shortest path weight from the given vertex v to the implicit destination vertex.
- Should be an admissible (it must not overestimate) heuristic.  If it fails to be admissible, A* is no longer optimal.  However see also <<bounded_relaxation>>.
- Should be monotone (aka consistent: For every edge (x,y): h(x) ≤ edge_weight(x,y)+h(y)).  A* can be implemented more efficiently by not making a node the current more then once (remember the above algorithm is for monotonic h).  Running such an A* algorithm on graph G is the same as running Dijkstra's algorithm on an alternate graph G' where edge_weight'(x,y) = edge_weight(x,y) + (h(y)-h(x)).
- *to-do*: must be constant (during the run-time of the algorithm)?
- Examples how to implement h: euclidean distance.

[[bounded_relaxation]]
_Bounded relaxation_: While the admissibility criterion guarantees an optimal solution path, it also means that A* must examine all equally meritorious paths to find the optimal path.  It is possible to speed up the search at the expense of optimally by relaxing the admissibility criterion.

Relation between different common naming schemes. spt_parent and and spwfs are my own abbreviations/terms; I prefer them over just parent and distance respectively since they more precisely say what they mean exactly.

[options="header"]
|=====
   |scheme 1  | scheme 2       | scheme 3 | spwfs' value           | is in outsideSptQ
.2+|unvisited | unvisited      | white    | INFINIT, upper bound   | yes 
              | tentative/open | gray     | < INFINIT, upper bound | yes
   |visited   | closed         | black    | final value            | no
|=====

[options="header"]
|=====
|scheme 1   | scheme 2       | comments
|spwfs      | d or distance  | actually an upper bound
|spt_parent | π or parent    |
|=====


[[Dijkstra]]
=== Dijkstra's algorithm
A greedy (can also be seen as dynamic programming) algorithm which solves the single-source <<shortest_path_problem>>, not allowing negative weights, in +O(E+V*lg(V))+ time and +O(V)+ space, assuming Fibonacci heaps are used. That notably includes directed or undirected graphs and graphs with cycles.

Terms and abbreviations: ubspwfs = upper bound on shortest path weight from source (in literature often called distance or simply d), SPT = shortest path tree. See also <<A_star>>.

Basic algorithm in pseudo code:
----------------------------------------------------------------------
Dijkstra(graph, source:vertex)
  create empty min priority queue outsideSptQ with ubspwfs as key
  for each vertex v in graph:
    v.ubspwfs = (v==source ? 0 : INFINITE)
    outsideSptQ.enque(v)
  while outsideSptQ not empty:
    current = outsideSptQ.deque() // greedely choose vertex and add it to SPT
    for each neighbor of current:
      relax(current, neighbor)
----------------------------------------------------------------------

Basic algorithm in prose: Out of the vertices not yet in the SPT, iteratively greedely choose vertex with the smallest ubspwfs (i.e. which is closest to the so-far SPT) and add it to the SPT, then update the ubspwfs for all it's neighbors.

Detailed algorithm:
----------------------------------------------------------------------
Dijkstra(graph, source:vertex [,dest:vertex])
  create empty min priority queue outsideSptQ with ubspwfs as key
  for each vertex v in graph:
    [v.spt_parent = NIL]
    v.ubspwfs   = (v==source ? 0 : INFINITE)
    outsideSptQ.enqueue(v)
  while outsideSptQ not empty:
    current = outsideSptQ.deque() // greedely choose vertex and add it to SPT
    [if current==dest return]
    for each neighbor of current:
      relax(current, neighbor, outsideSptQ)

relax(u:vertex, v:vertex, Q:MinPriorityQueue)
  alternate_ubspwfs_for_v = u.ubspwfs + weight(u,v)
  if alternate_ubspwfs_for_v < v.ubspwfs:
    Q.decreaseKey(v, alternate_ubspwfs_for_v)
    [v.spt_parent = u]
----------------------------------------------------------------------

More details in prose: Each node has an attribute ubspwfs. It is initially infinite, and each relaxation step potentially makes it smaller, i.e. potentially tightens the upper bound, until it reaches its final, true smallest value. It can be proven that the smallest ubspwfs in outsideSptQ, i.e. the one that the next dequeue call will return, has arrived at its true final spwfs value, and will never change again. Thus by choosing the vertex with the smallest ubspwfs, the algorithm always adds a vertex to the SPT which is not only closest, but in fact for which the spfws is known. Since the relaxation sets the spt_parent always to the current vertex, i.e. a node which already is in the SPT, when a vertex is dequed, its spt_parent is in the SPT, and thus it is implicitely added to the SPT.

Proof: Note that the shortest path problem has optimal substructere, see <<shortest_path_problem>>. *to-do*: Understand proof that the (upper bound) spwfs of the front vertex of outsideSptQ has arrived at it's true final value.

Notes:

- relax is called for all neighbors, even if they are already part of the SPT. Since their spwfs is already at the true final value, which can be proven, it will never be decreased anymore, so always calling relax doesn't hurt.
- Rational why negative weights are not allowed: The Dijkstra algorithm relies upon that adding a vertex to a shortest path does not decrease the shortest path's weight.
- Dijkstra can be seen as dynamic programming algorithm. DP(i) is the spwfs for vertex i: DP(i) = min~all predecessors j of i~{DP(j) + weight(j,i)}, with base case DP(startvertex)=0. See also https://en.wikipedia.org/wiki/Dijkstra%27s_algorithm#Dynamic_programming_perspective.

Relation to other algorithms: Dijkstra is a special case of the A* in that that A*'s h function is constant 0, but is a generalization of A* in that that it not only can solve the single-pair shortest path problem but also the single-source shortest path problem. Algorithm when implemented in terms of A*:

----------------------------------------------------------------------
function Dijkstra(graph, source:vertex)
    return Base(graph, source, NIL, lambda(v:vertex){0})

function Dijkstra(graph, source:vertex, dest:vertex)
    return Base(graph, source, dest, lambda(v:vertex){0})
----------------------------------------------------------------------

Analysis: Time +O(E+V*lg(V))+ if a Fibonacci heap is used to implement the min priority queue.  Using a binary heap it's +O((E+V)*lg(V))+, with an array it's  +O(E+V^2^)+.  Auxillary space: +O(V)+ -- however, the same space amount is also used for an answer which includes all shortest path weights and the shortest path tree.  If the answer must only include the weight of one target node the time and auxiliary space complexity remains the same.

Trivia: Etymology of the term `relaxation': There are two variants. 1) It comes from mathematics where relaxation means relaxing a constraint. Here the constraint is v.spwfs<=u.spwfs+weight(u,v). The smaller v.spwfs is, the less `pressure' there is to satisfy this constraint, thus the constraint is relaxed. 2) Think of the upper bound as an extended spring. Making the upper bound smaller relaxes the spring. The true value is reached when the spring is in its resting position. However from another viewpoint one could find it strange to call tightening an upper bound a relaxation.


=== Bidirectional search
It runs two simultaneous searches: one forward from the initial state, and one backward from the goal, stopping when the two meet in the middle.

*to-do*

[[Bellman_Ford]]
=== Bellman-Ford
A dynamic programming algoritm solving the shortest-path single source problem in O(|V|·|E|). Allows for negative weights (which Dijkstra doesn't) and can report negative cycles.

--------------------------------------------------
fun bellman-ford(G, source:vertex)
  init(G, source)
  |V| times:
    for each edge e in G:
      relax(e)
  [dedect-negative-cycles]

relax(e:Edge)
  for both directions of e:
    alternate_distance = e.v_from.distance + e.weight
    if alternate_distance < e.v_to.distance
      e.v_to.distance = alternate_distance
      [e.v_to.parent = e.v_from]

init(G)
  for each vertex v in G:
    v.distance = inf
    [v.parent = NIL]
  source.spwfs = 0

dedect-negative-cycles(G)
  for each edge e in G:
    if relax(e) would relax:
      abort, negative cycle found
--------------------------------------------------


[[floyd_warshall]]
=== Floyd–Warshall algorithm
A dynamic programming algorithm which solves the <<shortest_path_problem>> all pairs problem in O(|V|^3^). Negative weights are allowed, negative cycles are dedected.

Basic idea: You have a matrix C (V×V) storing shortest path for all vertex
pairs. Initially, no intermediate vertices are allowed, i.e. C corresponds to
the graph in its adjacency matrix representation. Then you iterate |V| times:
in each iteration, one more intermediate vertex is allowed, thus relaxing each
cell.

----------------------------------------------------------------------
Floyd-Warshall(G:GraphAsAdjacencyMatrix)
  C = G
  for k=1 to |V|:
    for i=1 to |V|:
      for j=1 to |V|:
        C[i][j] = min(C[i][j], C[i][k] + C[k][j]) // relaxation
----------------------------------------------------------------------

Explanation: c~ij~^k^ is a cell in the matrix in iteration k. It represents
the shortest path from v~i~ to v~j~, choosing intermediate vertices only from
the set {v~1~, ..., v~k~}. It's value is the min of:

- c~ij~^k-1^, i.e. directly going from v~i~ to v~j~, choosing intermediate
  vertices only from set {v~1~, ..., v~k-1~}.
- c~ik~^k-1^ + c~kj~^k-1^, i.e. going via v~k~, and both v~i~ to v~k~ and v~k~ to v~j~ choosing intermediate
  vertices also only from set {v~1~, ..., v~k-1~}.

*to-do* cycle dedection


[[Kruskal]]
=== Kruskal's algorithm
A greedy algorithm solving the <<MST_problem>> for a possibly disconnected graph in O(sort(E)+(E+V)α(V)) time, assuming an asymptotically optimal disjoint-set data structure is used internally, and where α is the very slowly growing inverse function of the Ackermann function. Rember that despite the name, it is actually a minimum spanning forest, not tree.

Basic algorithm in words: Start out with the MST being the empty list of edges; in other words, a forest where each vertex is one tree. Then iteratively add the smallest weighted edge to the MST iff that does not produce a cycle in the MST, since a by definition a tree is not allowed to have cycles.

Basic algorithm in pseudo code; see also <<disjoint_set>>:
--------------------------------------------------
KRUSKAL(G):
  foreach vertex v: MAKE-SET(v)
  order edges by weight, increasing
  foreach edge (u, v) in that ordered edge sequence:
    if FIND-SET(u) ≠ FIND-SET(v):
      add edge to MST, e.g. u.MSTparent = v
      MERGE-SETS(u, v)
--------------------------------------------------

Note that the running time depends heavily on the used sort algorithm used to sort the edge weights.  So if the weights are integers, a non-comparative sorting algorithm such as radix sort can be used.


[[Prim]]
=== Prim's algorithm
A greedy algorithm solving the <<MST_problem>> for a connected graph in O(E + V log V) time (i.e. the same as Dijkstra's algorithm), assuming a Fibonacci heap is used.

Basic algorithm. Note that it's quite similar to Dijkstra's shortest path algorithm.
--------------------------------------------------
MST-Prim(G:graph):
  for each vertex v: v.key = infinit, v.parent = NIL, v.inMST = false
  choose any vertex as root, and set root.key = 0
  create min priority queue outsideMstQ out of all vertexes, v.key being key  
  while outsideMstQ not empty:
    current = outsideMstQ.deque() // greedely add vertex to MST
    current.inMST = true
    for each neighbor of current:
      prim-relax(current, neighbor, outsideMstQ)

prim-relax(u:vertex, v:vertex, Q):
  if not current.inMST and weight(u,v)<neighbor.key
    Q.decreaseKey(neighbor, weight(u,v))
    neighbor.parent = current
--------------------------------------------------

Analysis:

- Initialization loop is done V times, and costs O(V) overall
- Overall prim-relax is called twice for each edge, i.e. O(E) times. prim-relax costs O(decrease-key), which is O(1) for a Fibonacci heap, so its O(E) overall.
- Outer loop is over all vertices, i.e V times. O(dequeue) is O(log V), so overall its O(V log V).
- All together its thus O(E + V log V)

== Concurrency related algorithms

=== Consumer producer

Solution using semaphores.  Allows for multiple producers and consumers.

----------------------------------------------------------------------
Semaphore emptyCount
Semaphore fullCount
Semaphore useQueue

produce:
  wait(emptyCount)
  wait(useQueue)
  putItemIntoQueue(item)
  signal(useQueue)
  signal(fullCount)

consume:
  wait(fullCount)
  wait(useQueue)
  item ← getItemFromQueue()
  signal(useQueue)
  signal(emptyCount)
----------------------------------------------------------------------

*to-do*:
- Solution with monitors
- Question: why isn't it in the above solution good enough to only guard the one critical section with a single binary semaphore?


=== Dining philosophers
*to-do*


== Misc algorithms

=== Horner's method / Horner's scheme

Task: Evaluate a polynomial P(x)=a~0~ + a~1~x + ... + a~n~x^n^ at x=x~0~.  Solution: Since the polynomial can be rewritten as a~0~ + x (a~1~ + x(a~2~+...+x(a~n~)...)) we can solve it beginning at the deepest level and iteratively go outward: b~n~=a~n~, b~n-1~=a~n-1~+x~0~b~n~, ..., b~0~=a~0~+x~0~b~1~ with b~0~ being the solution.  In code, with b~i~ stored in ++acc++umulator:

--------------------------------------------------
double polynomial(double x, const vector<double>& coefficients) {
    double acc = 0;
    for (int i=coefficients.size()-1; i>=0; --i) {
        acc = coefficients[i] + x * acc;
    }
    return acc;
}
--------------------------------------------------


[[hash]]
=== Hashes
A hash function +h(k,m)+ maps a key +k+ to an integer in the range [0,m), +m+ being an integer, and +U+ is the universe of possible keys. Typically k is an integer of the size of a CPU word. Thus prehashing is used to map any originial key of any size to [0,k).  

References:

- https://courses.csail.mit.edu/6.851/spring12/lectures/L10.html

====  totally random hash(table) (aka simple uniform hashing, SUHA)
An ideal hash is _totally random_ (aka assumption of _simple uniform hashing_, _SUHA_): Every k from U has the probability 1/m to be mapped to slot s, for every slot s in [0,m). But that requires Θ(U log m) space (a table of U entries, each storing the hash functions's output, i.e. each entry needs Θ(log m)space), which is in general too much.

A _totally random hash table_ T is the lookup table implementing a totally random hash function, i.e. h(x)=T[x].


==== universal familiy
A family of hash functions H is _universal_ if for every h∈H, and for all x≠y ∈ U, Pr~h~{h(x) = h(y)} = O(1/m).

Examples:

- ++h(x) = [(ax) mod p] mod m++ for 0<a<p, wheras p is a prime. p should be larger than m, else the higher slots are unused. ax can also be vector dot product. Note that this is not exactly the same as the division method.

- ++h(x) = (a · x) >> (lg u − lg m)++. Use lg m high order bits of product ax.

==== strong universal family
A famility H of hash functions is _strong universal_ if for all x,y ∈ U, x≠y, Pr~h~{h(x) = h(y)} ≤ 1/m.

[[k_wise_independent]]
==== k-Wise Independent family
A family H of hash functions is k-wise independent if for every h ∈ H, and for all
distinct x~1~, ..., x~k~ ∈ U, Pr{h(x~1~)=t~1~ and ... h(x~k~)=t~k~} = O(1/m^k^).

Example pair-wise independent (k=2):

++h(x) = [(ax + b) mod p] mod m++ for 0<a<p and for 0≤b<p. Here, again, p is a prime greater than |U|.

==== simple tabulation hashing
View key x as vector of characters x~1~, ..., x~c~. We create a totally random hash table T~i~ on each character. ++h(x)=T~1~(x~1~) xor ... xor T~c~(x~c~)++

Is <<k_wise_independent,3-wise independent>>.

==== division method
+h(k,m)=k mod m+. In practice not so bad if m is prime and not close to a power of two. Still pretty `hackish'.  Rational for m being prime: When m has common factors with k, the effectively used table size gets divided by the product of those factors: ++k=k%m=(a*f1*f2)%(b*f1*f2)=a%b)++


==== multiplication method
++h(k,m,a,w) =((ak) % 2^w^) >> (w-r)++, where as m=2^r^, i.e. +r=lg(m)+, and the machine stores integers in words of size w bits. a should be odd and not close to a power of two, between 2^r-1^ and 2^r^.

Intuitively: The multiplication mixes up bits, especially in the area from bit (w-r) to w, so we take that area: +% 2^w^+ cuts away the part left of bit w, the shift right cuts way the bits right of bit (w-r).



[[ADT]]
== Abstract data types (ADT)
An abstract data type is defined only by the operations that may be performed on it and by mathematical pre-conditions and constraints on the effects (and possibly cost) of those operations.  In OO lingo, it is an interface.  See also <<data structures>>,  which in OO are (non-abstract) classes.

=== Summary

*to-do*: finish tables

*to-do*: combine header cells , e.g. queue and stack are specialized deques

linear collections, excluding priority queues
|=====
|               | list | array | deque | queue  | stack
|insert-at(iter)| x    |       |       |        |      
|insert-front   | x    |       | x     | x      | x
|insert-back    | (x)  |       | x     |        |
|find(pos)      |      | x     |       |        |
|find-front     | x    | x     | x     |        | x
|find-back      | (x)  | x     | x     | x      |
|delete-front   | x    |       | x     |        | x
|delete-back    | (x)  |       | x     | x      |
|delete(iter)   | x    |       |       |        |
|successor/pred.| x    | x     |       |        |
|=====

associative collections and ordered by a key, plus priority queues
|=====
|               | priority queue | BST
|insert         | x              | x
|find(key)      |                | x
|find-min       | x              | x
|find-max       |                | x
|delete-min     | x              | x
|delete-max     |                | x
|delete(key)    |                | x
|successor/pred.|                | x
|=====

// associative unordered collections
// |=====         | set | 
// |insert
// |find(value)
// |delete
// |=====

*to-do*: draw is-specialization/generalization DAG plus data structures implementing them


[[collection]]
[[container]]
=== Collection (aka Container)
Grouping of data items.  Generally, the data tiems will be of the same type.

Common operations: Create empty container, report number of objects it stores (size), delete all its objects (clear), insert new objects, remove objects, provide access to stored objects.

[[linear_collection]]
.Linear collections
The elements form a sequence. Example ADTs: <<list_adt>>, <<stack>>, <<queue>> (<<priority_queue>> [not associative since only the min element can directly be accessed], <<deque>>, <<depq>>)

[[associative_collection]]
.Associative collections (sorted or unsorted)
Given a key, the collection yiels a value. Example ADTs: <<associative_array>> (<<set>> [value being the key] (<<multiset>>))

.Graphs
Data items have associations with one or data items in the collection. Example ADTs: <<tree_adt>>.


Notably usually not considered a collection: fixed-sized arrays


[[array_data_type]]
=== Array data type

Random access, fixed size.

Implementation: array data structure


[[list_adt]]
=== List (aka sequence)

Sequencial access (no random access)

Implementations: linked list, doubly linked list, array data structure


[[map]]
[[associative_array]]
[[dictionary]]
=== Associative array (aka map, symbol table, dictionary)
<<collection>> of (key, value) _pairs_ (aka _items_), such that each key appears at most once in the collection.  Specialization of <<multimap>>.

Operations: _insert_ (aka add) a pair, _delete_ (aka remove) a pair, _look-up_ (aka search, find) value associated to a given key.  Optionally also _iterate_ over all pairs, _modify_ (aka reassign), the value of an already existing pair.

Implementations: association list, hash table, binary search tree, radix trees, tries, Judy arrays, ....


[[multimap]]
==== Multimap (aka multihash)
Is a generalization of a <<map>> (aka associative array) in which more than one value may be associated with a given key.  My words: As with <<multiset>>s, this is used in two distinct senses: either equal values are considered identical, and are simply counted, or equal values are considered equivalent, and are stored as distinct items.


[[bag]]
[[multiset]]
==== Multiset (aka bag)
A specialization of an <<associative_array>> in that the value part of the associative array's (key, value) pairs is absent or a sentinel value (like 1).

A generalization of a <<set>> in that it allows duplicates.  This is used in two distinct senses: either equal values are considered identical, and are simply counted, or equal values are considered equivalent, and are stored as distinct items.


[[Set]]
==== Set
A specialication of a <<multiset>> (which in turn is a specialization of an <<associative_array>>), in that no duplicates are allowed.


[[deck]]
[[dequeue]]
[[deque]]
=== Double-ended queue (aka dequeue, deque, deck)
<<linear_collection>> where elements can only be inserted to and removed from either side of the sequence.  Is a generalization of a <<queue>> and a <<stack>> in that elements can be inserted and removed to/from both sides.

Implementations: <<circular_buffer>> which resizes when it's full. <<dynamic_array>>, placing the current elements in its middle, and resize when either side becomes full.

Implemented more specialized ADTs: <<collection>>.

Terminology: Deque is the abbrevation of double-ended queue.  Deque (pronounced deck) is the abbbreviation thereof.  Deck is as in an deck of cars, which also provides a good mental image.

See also: - http://www.codeproject.com/Articles/5425/An-In-Depth-Study-of-the-STL-Deque-Container
- C&plus;&plus;'s deque allows random access/insertion, is thus pretty similar to vector. vector vs deque discussions: http://stackoverflow.com/questions/5345152/why-would-i-prefer-using-vector-to-deque, http://www.gotw.ca/gotw/054.htm


[[depq]]
==== Double-ended priority queue (aka depq or double-ended heap)
*to-do*


[[queue]]
=== Queue
<<linear_collection>> where the element removed is prespecified by a first-in-first-out (FIFO) policy.  Is a specialization of a <<deque>> in that insertion is only allowed on one side and removal only on the other side.

Common operations: Elememts can only be added to its _tail_ side (_enqueue_), and only be removed from the other side called _head_ (_dequeue_).  The only element that can be accessed is the one on the head side (_front_ or _peek_).

Common implementations offer +O(1)+ time and +O(1)+ auxiliary space for these operation and +O(n)+ space for the collection aspect.

Common implementations: circular buffer, doubly linked list, singly linked list with an additional pointer to the last node

Implemented more general ADTs: <<collection>>, <<deque>>


[[priority_queue]]
==== Priority Queue
A min (max) priority queue is similar to a queue, however dequeue extracts the element with the max (min) key.  I.e. each element has a key.  Principal operations for a max-(min-)priority queue: _insert_ (aka _enqueue_), _dequeue_ (aka _extract-max_(__-min__)), _peek_ (aka _max_(_min_)), _increase-key_(_decrease-key_).

Sorting and priority queues: If it is possible to perform integer sorting in time T(n) per key, then the same time bound applies to the time per insertion or deletion operation in a priority queue data structure (Thorup 2007.  It's however a complicated reduction).  *to-do*: elaborate more on relation sorting to priority queues

Common implementations: <<heap>>, self-balancing binary tree


[[stack]]
=== Stack
<<linear_collection>> where the element removed is prespecified by a last-in-first-out (LIFO) policy.  Is a specialization of a <<deque>> in that insertion and removal are only allowed on one single side.

Main operations:  Insertion is often called _push_ and can be only to one side called _top_.
Removal is often called _pop_ and can only be the element at the top end.  The only element that can be accessed is the one on the top end of the stack (_top_ or _peek_).

Implementations: <<array>>, <<linked_list>>.


[[tree_ADT]]
=== Tree
Note that there is a distinction between a tree as 1) an abstract data type,  2) a data structure and 3) a topic in graph theory.

Terms (see also those of <<graph>>, in particular <<tree_graph>> and <<DAG>>):

- _siblings_: nodes with the same parent.
- _cousins_: nodes with the same grand parent.
- _internal node_: A node with at least one child.
- _external node_ (aka _leaf_): A node with no children.
- _degree_: Number of sub trees of a node
- _level_: Depth + 1
- _size_: Number of nodes
- _height of tree / node_: Informally: Largest distance (see <<graph>>) between root / that node and any leaf. Formally: ++height(node) = max(height(node.left), height(node.right)) + 1++, whereas height of NULL is -1 (equalently: height of leaf node is 0). Height of tree, height of root, depth of deepest leaf are all synonymous.
- _depth_ of node: Distance from root to that node.

Implementations: See those of <<graph>>,  and the methods for storing a <<binary_tree>>


[[binary_search_tree]]
=== Binary Search Tree (aka BST, ordered/sorted binary tree)
Is a specialized <<binary_tree>> where 1) each node has a comparable key 2) for each node: the key of the left child, if child present, is smaller-or-equal than the node's key, and the key of the right child, if present, is larger-or-equal than the node's key.  Be +n+ the number of elements.  +h≥lg n+ the height of the tree.  The expected height is +h=lg n+ for a randomly built binary tree.

_binary search tree property:_ If node +y+ is in the left subtree of node +x+, then +y.key<=x.key+, if +y+ is in +x+'s right subtree, then +y.key>=x.key+.

_Search_ key +k+: +O(h)+. Recursively or iteratively, for current node +x+, if +k<x.key+ continue with left subtree, else right subtree.

[[rotation]]_Left/right rotation_: +O(1)+. Preserves the order of elements of an in-order traversal. Note that thus, in case of an BST, also preserves the binary-search-tree property.
+
The following visualizes left/rifght rotation. x/y/r are nodes, A/B/C are subtrees.  The middle subtree B changes the parent from x/y to y/x, and x/y swap parent/child relation which includes that the new parent's parent must be changed to r.
+
----------------------------------------------------------------------
   r                 r
   x      left       y  
A     y     →     x    C
     B C    ←    A B
          right    
A  x ByC         AxB y C
----------------------------------------------------------------------

_Min_/_Max_: +O(h)+. Follow left/right subtree until the leaf is reached.

_Successor_/_Predecessor_: +O(h)+. **To-do**

Implementations: <<AVL>> trees, red-black trees

Implements these more general ADTs: <<associative_array>>


[[disjoint_set]]
=== Disjoint-set (aka union-find, merge-find set)
A collection of n elements, partitioned into a number of disjoint sets. Or from another point of view: Given an undireced graph of n vertices, keeps track of connected components, and thus can answer which vertices are connected.

Usually each set chooses one of its elements as the representative; that representative element identifies the set. It is undefined which element is chosen, but it stays the same as long as the data structure is not modified.

Main operations:

- make-set(v): Adds element / vertex v to the collection, as a new set containing only that element.
- find-set(v): Returns the id of the set / connected-component element / vertex v is in. To see if elements / vertices u and v are in same set / connected: find-set(u)==find-set(v).
- merge-sets(u,v): Merges the sets of elements u and v / adds edge between vertex u and vertex v. It is undefined what the id of the new set is.

Implementations: <<disjoint_set_linked_list>>, <<disjoint_set_forest>>

|============
|                  | linked-list | forest / union by rank | forest / union by rank + path compression
| insert(v)        |             | O(1)                   | O(1)
| find-set(v)      | *to-do*     | O(log n)               | O(log* n)
| merge-sets(u,v)  |             | O(log n)               | O(log* n)
|============

Applications: <<Kruskal>>'s algorithm, *to-do*

Trivia: Was invented specifically to make Kruskal's algorithm more efficient


[[graph]]
=== Graph
In the following, no distinction is made between the term graph referring to an specific abstract data type and the term graph referring to a topic in mathematics.

A _graph_ +G=(V,E)+ is given by a set of _vertices_ +V+ (aka _nodes_) and a set of _edges_ +E+, each edge being an pair of elements from +V+.  A _multigraph_ is one where +E+ is a multiset (as opposed to just a set).  Alternatively: a multigraph is allowed to have _parallel edges_, that is, edges that have the same start/end vertex.  In an undirected graph the two vertices of an edge are said to be _adjacent_ to each other; in an directed graph only the dst vertex of an edge is adjacent to the src vertex.  Two edges are called _incident_ if they share a vertex.  An _undirected graph_ is one in which edges are an unordered pairs; the edge ++(a,b)++ is identical to the edge ++(b,a)++. An _directed graph_ (aka _digraph_) is one in which edges are ordered pairs and are also called _arcs_, _directed edges_ or _arrows_.  A _loop_ is an edge which starts and ends on the same vertex.  _Link_ is an edge with two different ends.  A _simple_ graph is an undirected graph that has no loops and at most one edge between any two different vertices.  A _sparse_ graph is one for which +|E|+ is much less than ++|V|^2^++.  A _dense_ graph is one for which +|E|+ is close to ++|V|^2^++.  A _vertex cover_ is a set of vertices such that each edge of the graph is incident to at least one vertex of the set.  An _independent set_ (aka _stable set_) is a set of vertices, no two of which are adjacent.

A _path_ (aka _walk_, however Wikipedia says that path commonly refers to an open walk) in an undirected graph is an ordered sequence of vertices {v~1~,...,v~n~} such that v~i~ is adjacent to v~i+1~.  A _closed walk_ is one where the first and last vertices are the same, an _open walk_ is one where they are different.  A _trail_ is a path in which all edges are distinct.  A _simple path_ does not have any repeated vertices.  The _path weight_ is the sum of the weights of its constituent edges.  The _length_ of a path is sometimes defined as the number of edges on the path or as synonym to path weight.  The [[shortest_path]]_shortest path_ from vertex u to v is any path with minimal path weight.  See also <<shortest_path_problem>>.  The _shortest path weight_ is the path weight of the shortest path; defined to be infinite if there is no path, and often defined as -infinite it contains a negative-weight cycle.  Any sub path of a shortest path is itself a shortest path.  The _distance_ between two vertices is the length of the shortest path.  A _(pre/in/post)order tree walk_ does the key action with the current node's payload before/between/after recursively calling the children.

Two vertices v and u are called _connected_ if there is a path from v to u, otherwise they are called _disconnected_.  A _connected graph_ is one where every pair of vertices is connected.  A directed graph is considered a connected graph if for every pair of vertices there is a path in either direction.  A _weakly connected_ graph is a directed graph which would be a connected graph if the edges were taken to be undirected.  A directed graph is _strongly connected_, if for every pair there's a path in both directions.  A _connected component_ (or just _component_) of an undirected graph is a subgraph which is connected and is not connected to any vertices of the supergraph.  A directed graph is called _strongly connected_ (or _strong_) if it contains a path from u to v and from v to u for every pair of vertices u and v.  A _strongly connected component_ of a directed graph G is a subgraph that is strongly connected, and is maximal with this property: no additional edges or vertices from G can be included in the subgraph without breaking its property of being strongly connected.  If each strongly connected component is contracted to a single vertex, the resulting graph is a directed acyclic graph, the _condensation_ of G.  A _cut_ (aka _vertex cut_, or _separating set_) of a connected graph G is a set of vertices whose removal renders G disconnected.  A _complete graph_ with n vertices, denoted Kn, has no vertex cuts at all, but by convention κ(Kn) = n − 1.  The _connectivity_ (or _vertex connectivity_) κ(G) (where G is not a complete graph) is the size of a minimal vertex cut.  A graph is called _k-connected_ (or _k-vertex-connected_) if its vertex connectivity is k or greater.  A _clique_ in an undirected graph is a subset of its vertices such that every two vertices in the subset are connected by an edge.

References:

- http://en.wikipedia.org/wiki/Glossary_of_graph_theory


[[DAG]]
A _directed acyclic graph_ (_DAG_) is a graph with no directed cycles.  Given a directed edge C->P, C is the _child_ and P the _parent_. Nodes reachable from C are C's _ancestors_, nodes which can reach P are P's _descendants_.  Whether or not a node is it's own ancestor/descendant is not defined across literature. CLRS say yes.  The _lowest common ancestor_ (_LCA_, aka, less accurately, _least common anchestor_) of two nodes x and y is the first common ancestor in topo order.

[[tree_graph]]
See also <<tree_ADT>>. A _tree_ is an undirected graph in which any two vertices are connected by exactly one path (which then naturally is a simple path).  Alternatively: an undirected connected acyclic graph.  Thus it's always undirected, acylic, connected and bipartite.  Note that an arborescence is not a connected graph (consider a root with two childs; the two childs are not connected).  In informatic context, the term tree is often used for what we call here an directed rooted tree.  A _rooted tree_ is a tree in which one vertex has been designated the root.  An _arborescence_ (aka _directed rooted tree_ or _out-tree_) is a directed rooted tree in which all edges point away from the root.  An _anti-arborescence_ (aka _in-tree_) is an arborescence where all edge directions are reversed.  A _forest_ is a disjoint union of trees.  Alternatively: an acyclic undirected graph.  An _ordered tree_ (aka _plane tree_) is a rooted tree in which an ordering is specified for the children of each vertex.  A _poly tree_ is a directed acyclic graph which would be a tree if it's edges were undirected.  A _spanning tree_ of an undirected graph G is a subgraph that is a tree which includes all of the vertices of G.  A [[MST]]_minimum spanning tree_ (aka _MST_) of is a spanning tree of G with the minimal total weighting for its edges.  See also the <<MST_problem>>.  The equivalent to a MST in an directed graph is a _spanning arborescence of minim weight_ (aka _optimum branching_).

A _flow network_ (aka _transportation network_) is a directed graph where each edge has a _capacity_ and a _flow_ which can no exceed the capacity.  The amount of flow into a node must equal the amount of flow out of it, unless the node is a _source_ or a _sink_. _maximum flow problems_ involve finding a feasible flow through a single-source, single-sink flow network that is maximum.  It can be seen as a special case of the more complex problems, e.g. the circulation problem.   The _max-flow min-cut theorem_ states that the maximum flow is equal to the minimum capacity over all possible s-t edge cuts.  An s-t edge cut is an edge cut such that one of the resulting component contains the source and the other component contains the sink.  The capacity of an edge-cut is the sum of the capacities on the cut edges.  Solutions of max-flow: _Ford–Fulkerson algorithm_ +O(E*f)+ for the case where capacities are integers, whereas f is the maximum flow.

Common problems: _single-pair shortest path_ problem: from single source to a single destination, _single-source shortest path_ problem: from a single source to all others, _single-destination shortest path_ problem: form all others to a destination, _all-pairs shortest path_ problem: from all to all.  The chapter <<NP_complete>> also lists some graph problems.

Common implementations:

- _Adjacency list_: Typically for sparse graphs.  Collection of unordered vertex lists, one for each vertex.  Sub-forms of how to implement an adjacency list:
 * An associative array associates each vertex (being the key) to an unordered list (being the value) of its adjacent vertices.  For the associative array, often a hash table is used.  If the key can be an integer, e.g. when the vertices are enumerated, then a simple array can be used instead the associative array.
 * Object graph of vertices: Each vertex has a collection of pointers to its adjacent vertices. Optionally, each element in that collection, actually representing an outgoing edge, also stores other properties of the edge, e.g. its weight.  However note that for undirected graphs, each edge is stored redundantely twice. *to-do*: why is apparently the associative array before much more common than this graph variant?
 * Object graph of vertices and edges: Each vertex object has a collection of pointers to its outgoing edges.  Each edge object has a pointer to its start and end vertex.
- _Edge list_: A collection of edge objects, each edge object storing something to identify the start and end vertex, possibly additionally also the edge's weight.
- _Adjacency matrix_ |V|×|V|:  Rows represent source vertices and columns represent destination vertices and cells the associated edge.  Data on vertices typically stored externally.  Typically for dense graphs, or when a quick way is needed to tell if two vertices are adjacent.  Does not work for multigraphs. *to-do* symetric for undirected graphs, inf for not adjacent vertexes, edge weights...
- _Incidence matrix_ |V|×|E|: The rows represent the columns, the columns the edges, a cell is 1 if the associated vertex is an start point of the assiciated edge, -1 if it's the end point, and 0 otherwise.  In a weighted graph, the 1s are replaced by the edge's weight.


[[data_structure]]
== Data structures
A concrete particular iway of organizing data in memory.  In OO lingo, its is a (non-abstract) class.  See also <<ADT>>, which is in OO lingo an interface.


[[table]]
[[array]]
=== Array data structure (aka table)
Fixed size, +Θ(1)+ time for indexing, with a very low constant factor.  ++O(0)++ wasted space.  Due to the fixed size, elements cannot be added / removed.


[[dynamic_table]]
[[dynamic_array]]
=== Dynamic array (aka array list, dynamic table, resizeable array)
In contrast to <<array>> the size is variable, thus allows elements to be added / removed.  _Capacity_ is the number of elements the container could currently hold, and the _size_ is the number of elements it actually currently contains.

[[table_doubling]]
==== Table doubling
When size=capacity upon an insertion, create a new table with double the capacity and copy all elements over -> insertions are +Θ(1)+ amortized.  Upon deletions, when you don't mind slack, never resize the table (as the STL does), or half the capacity when size drops below capacity/4. In that case both insertions and deletions are +Θ(1)+ amortized. (You can't half the capacity when the size reaches half the capacity because in a sequence like inserting/deleting/inserting/deleting, each operation could encompass a table resize which would mean +O(n)+ per operation.)  Of course, other constants than 2 can be used, as long as the factor which is to do shrink is greater than the factor to enlarge.

One can get +Θ(1)+ by roughly this idea: When you remark that you start to get full, start a new table with a larger capacity, initially empty.  On each insertions operation, copy a constant amount of items from the old table to the new one.  Once the old table is really full, just switch over to the new table.  All in all it's quite complicated, so it's not that often used.


[[linked_list]]
=== Linked list

Implementation of the ADT <<list>>.

Orthogonal properties:

- Singly, Doubly or Multiply linked
- Circular linked yes/no
- Sentinel nodes yes/no


[[circular_buffer]]
=== Circular buffer (aka cyclic buffer, ring buffer, circular queue)
Uses a single, fixed-size buffer as if it were connected end-to-end.

Internally uses 1) an array which's size equals circular's buffer capacity, 2) an pointer (or index) to the first element and 3) one to the last element.  Pointers in a circular buffer wrap around at the underlying array border (array.first and array.last (according array.size=circular_buffer.capacity)).

Implements the ADT <<queue>>

Difficulties:

- Depending on the exact implementation, distinguish the case that the buffer is empty and that it is full is not possible, because in both cases start and end point to the same element.


=== Direct-address table
Implements the <<associative_array>> ADT.  An array of size |U|, where U is the universe, i.e. the set of possible keys.  A key's value is the index into the array where the data corresponding to the key is stored.

Time: +O(1)+ worst average best case.  Space: +O(|U|)+.


[[hash_table]]
=== Hash tables
Implements the <<associative_array>> ADT.  Is an array of size m. A <<hash>> function +h(k,m)+ is used to map a key +k+ to [0,m), i.e. to an index into the table. When two keys hash to the same slot that is called a _collision_. The following subchapters describe ways how to deal with collisions. +α=n/m+ is the _load factor_ of the table.

In general, the various variants have the following properties: Search/insert/delete time in +O(k)+ for the best and average case, and ++O(k+n)++ worst case. Space is usually +O(n)+.

==== Collision resolution with (separate) chaining
Each table slot has associated a sequence of items, typically a singly linked list. The expected chain length is the table's load factor.

Insert/delete/find: +Θ(1)+ (Actually +Θ(1+loadfactor)+, but when loadfactor=O(1) (i.e. m=Ω(n)), it becomes +Θ(1)+). Rational: Paying O(n) to find table slot, then O(loadfactor) to walk the list.

Loadfactor should be +Θ(1)+ (i.e. m should be +Θ(n)+). If +m+ is too small, the loadfactor is too high, in the worst case not +Θ(1)+ anymore.  That would lead to hash table operations not being +Θ(1)+ anymore.  If +m+ is too large, we waste space.

==== Collision resolution with open addressing
Each slot can really only take one key, and has an attribute whether it's free. If a hash maps a given key to an non-free slot, a probe sequence is used iteratively to ultimatively find a free slot. Typically delition and table resize are possible but complicated, since *to-do*.  Unlike with chaining, if all slots are used, the table must be enlarged, see also <<table_doubling>>.

Probe sequences. +h(k,m,i)+ is the same as +h(k,m)+, with the aditional parameter i, denothin the i-th probe. If h(k,m,0) returns a used slot, you try h(k,m,1) and so on.

Linear probing:: ++h(k,m,i) = (h(k,m)+i) mod m++. Good locality, but most sensitive to primary clustering.

Quadratic probing:: Try m1=m0+1, m2=m1+2=m0+3, m3=m2+3=m0+6. Properties between linear probing and double hashing

Double hashing:: ++h(k,m,i) = (h(k,m)+i*h2(k,m)) mod m++. Interval is computed by another hashfuncion. Bad locality, but exhibits virtualy no clustering. m is typically a power of two. If m is even, h2 should deliver an odd number, else every 2nd slot will never be probed.


==== Perfect hashing (FKS)
*to-do*


==== Cuckoo hashing
*to-do*


=== Association list
Is an implementation of the ADT <<associative_array>>.

*to-do*


=== Selection problem
See <<order_statistics>>


=== Nearest neighbor search (NNN)

*to-do*:


[[binary_tree]]
=== Binary tree
A <<tree>> data structure in which each node has at most two children.  Note that a <<binary_search_tree>> is something else with more restrictions.

Properties:

- _full_(aka _proper_): Every node other than the leaves has two children.
- _perfect_: (aka ambiguously (see next) complete): A full binary tree in which all leaves have the same depth
- _complete_: Every level, except possibly the last, is completely filled, and all nodes are as far left as possible.
- [[balanced]]_balanced_: height is +Θ(lg |V|)+
- [[weight_balanced]]_weight balanced_: The size difference between the left and the right subtree is kept within some constant factor.
- _degenerated_ (aka _pathological_): Each node has at most one child.  The tree is thus effectively a linked list.

Methods of storing:

- See <<graph>>
- As an implicit data structure in an array.  Be i the current node's index, 0 the first index, then its parent is at index floor((i-1)/2), its right child at 2i+1 and its left child at 2i+2.  In the case of a complete binary tree, no space is wasted.  See also <<binary_heap>> which commonly uses this scheme.


[[naive_BST]]
=== (Naive) binary search tree (data structure)
A data structure implementing the binary search tree ADT. When inserting, the elements are always inserted as leaves, whithout changing previous nodes.


[[random_binary_tree]]
=== Randomized binary search tree
Randomly permute the input before building the <<naive_BST>>. 

*to-do*: 

Expected height E[height]=O(lg(n))

[[avl]]
=== AVL Tree
A data structure implementing the binary search tree ADT.  Is a <<balanced>> binary search tree; balance is ensured by the following invariant: For each node n: |height(n.left) - heigh(n.right)| ≤ 1.  From that (indirectly) follows: tree height ≈ 1.44 lg(|V|).

Time complexity: O(log n) average and worst case for all basic operations (search, insert, delete).

Space complexity: O(n)

Each node stores its _balance factor_, which is the difference in height of the left and right subree. Must be in range [-1,1].

Rough description of how insertion/deletion work:

1. First do a normal BST insertion or deletetion (which honor the BST property)
2. For each node on the path from the newly inserted node up to the root: if balance factor is not in range [-1,1], fix it by only _rotation_ operations.

See also <<search_tree_comparison>>


[[red_black]]
=== Red-Black
A data structure implementing the binary search tree ADT.  Is a <<balanced>> binary search tree; Balance is preserved by attributing each node with one of two colors (typically called `red' and `black') in a way that satisfies red-black properties (see below).  Tree height ≈ 2*lg(|V|).

red-black properties:

- Roots and NILs are black (typically NILs are called the leaves and all other `poper' nodes are called internal nodes).
- Every red node has a black parent (i.e. never two consequtive red nodes on a simple path)
- For each descendant of a node n, the number of black nodes on the simple path from n to descendant is the same

Time and space complexity: save as <<AVL>> tree.

See also <<search_tree_comparison>>


=== 2-3 tree
*to-do*


=== 2-3-4 tree
*to-do*


=== B-tree
*to-do*


=== B+ tree
See data_base_systems.txt


[[vEB_tree]]
=== Van Emde Boas tree (aka vEB tree)
Is a tree data structure implementing the ordered <<associative_array>> ADT with m-bit integer keys. It performs all operations in O(lg m) time. The vEB tree has good space efficiency when it contains a large number of elements


[[cartesian_tree]]
=== Cartesian tree
A binary tree having the heap property and having the additional porperty that its in-order traversal delivers a given sequence S. Can be built in O(n) time from S and vice versa.

Example:
--------------------------------------------------
S = [8,7,2,8,6,9,4,5]

T =      2
        / \______
       7         4
      /       __/ \
     8       6     5
            / \
           8   9
--------------------------------------------------

Building cartesian tree T from a sequence S, which assumes parent pointers: To process each new value x, start at the node representing the value prior to x in the sequence and follow the path from this node to the root of the tree until finding a value y smaller than x. This node y is the parent of x, and the previous right child of y becomes the new left child of x. 

Building sequence S from cartesian tree T: If T's nodes are not labeled with values as above, which is likely, label each node with it's depth. Do an inorder traversal, resulting in an array of node labels.  LCA on T is still the same as RMQ on S.

Applications:

- <<RMQ>> in S corresponds to <<LCA_problem,LCA>> in T.
- Range searching *to-do*


=== Skip list
Is a data structure implementing the ordered <<associative_array>> ADT. Search, insert, delete in +O(lg n)+ time with high probability (opposed to `only' on average)


=== Treap (aka priority search tree)
A balanced (with high probability) binary search tree. The idea is to use randomization and the heap property to maintain balance with heigh probability, i.e. balancedness is not guaranteed. Search, insert, delete in O(log n) time with high probability, but O(n) worst case.

Each node in the this BST additionally has a priority, which is assigned a random value upon insertion. Upon insertion/deletion, both the BST invariant regarding the key and the heap property regarding the priority have to be fullfulled. This is done by a normal BST insert using the key, and then do rotations until the heap property is fullfulled regarding the priorities.

Trivia: The name is a portmenteau of tree and heap.

=== kd tree
A binary search tree ... *to-do*


=== splay tree
*to-do*


[[search_tree_comparison]]
=== Search tree comparison

<<AVL>> tree vs <<red_black>> tree: theoretically equivalent since time and space complexity are identical.  AVL trees are more rigidly balanced (≈ 1.44 lg(|V|)) than red-black trees (≈ 2 lg(|V|)), whereas the number of rotations when inserting or deleting is O(lg n) for AVL and O(1) for red-black.  Followingly prefer AVL when number of lookup operations dominate sum of insert/delete operations, and red-black oth erwise.

*to-do*: B-trees for 2ndary memory


[[heap]]
=== Heap
A _heap_ is a specialized tree-based data structure that satisfies the _heap property_: If node A is a parent node of B, then the key of node A is ordered with respect to the key of node B with the same ordering applying across the heap.  In a _max heap_ the parent node key is greater than or equal to those of the children, in a _min heap_ it's smaller than or equal.  Thus the element with the largest (max heap) / lowest (min heap) key is always stored at the root.  Note that there is no implied ordering between siblings or cousins.

Time complexities for binary, binomial, Fibonacci, pairing, Brodal, rank pairing, strict Fibonacci:

Creation::
- create-heap: create an empty heap
- make-heap (aka build-heap aka heapify): create a heap out of given elements. +O(n)+ binary, others *to-do*.
- union (aka merge): +Θ(m lg(n+m))+ binary, +O(lg(n))+ binomial, +Θ(1)+ others

Inspection::
- min (max) (aka peek or find-min/max): +Θ(1)+
- size()

Modification::
- extract-min(-max) (aka pop): +O(lg(n))+
- insert: +Θ(lg(n))+ binary, +Θ(1)+ others
- decrease-(increase-)key: +Θ(lg(n))+ binary & binomial & pairing,  +O(1)+ others

Applications of heaps:

- The heap data structure is one maximally efficient implementation of the <<priority_queue>> ADT.
- Merge sort
- Dijkstra's shortest-path algorithm
- Order statistics

A heap data structure should not be confused with `the heap' which is a common name for the pool of memory from which dynamically allocated memory is allocated.


[[binary_heap]]
==== Binary heap
In a _binary heap_ the tree is a complete <<binary_tree>>. *to-do*: study implementation of the basic operations.

--------------------------------------------------
        0
    1       2
  3   4    5  6      0123456789 array index
 7 8 9               01-2---3-- tree level
--------------------------------------------------

parent(i) = floor((i-1)/2)
right-child(i) = (i+1)*2 - 1
left-child(i) = (i+1)*2


- +heapify(i)+.  Assumes that children of node +i+ are max heaps, but +i+ might violate the heap property.  Time: +O(lg(nst))+, where nst are the number of nodes in the sub tree rooted at i.
- +build_heap()+:  Converts an array into a heap.  Common implementation: in a bottom-up manner, for each node, starting at one-before-leaf-height height, call +heapify+.  Time: +O(n)+.


==== Fibonacci heap
*to-do*:


[[tries_comparision]]
=== Tries / suffix tree/arrays compari son

Data structure used to store a node (having up to Σ childs), and the resulting querry time and space needed, both in the trie (as opposed to the node).

|=======
|                               | query     | space      | ordering
| c. trie / array               | O(P)      | O(k·Σ)     | ✓
| c. trie / BST                 | O(P·lg Σ) | O(k)       | ✓
| c. trie / weight balanced BST | O(P+lg k) | O(k)       | ✓
| c. trie / w.b. BST + trimming | O(P+lg Σ) | O(k)       | ✓
| c. trie / van Emde Boas       | O(P)      | O(k·lglgΣ) | ✓
| c. trie / hash table          | O(P)      | O(k)       | ✗
| suffix tray                   | O(P+lg Σ) | O(k)       | ?
| suffix array                  | ?         | ?          | ?
|=======


[[trie]]
=== Trie (aka digital tree, radix tree, prefix tree)
See this as an intro to what a trie is. In practice you will use a <<compressed_trie>>, see there for more details.

Is an implementation of the ordered <<associative_array>> ADT. It stores +k+ items (key/value pairs), the stored keys being strings _T~i~_, i=1,...,k, the strings' letters are from alphabet _Σ_.  Internally, there is also the special letter _$_ which represents the end of a string.  A trie is a rooted tree where each edge is labeled with a letter.  Thus child nodes have an order.  A root-to-leaf path represents a string/key, the so reached leaf node the value associated with that key.  The strings derived from paths root to leaf are called _words_, the strings from other paths are called _prefixes_. |T|=|T~1~|+...+|T~k~| is the max number of nodes stored in the tree. In query/lookop, _P_ is a pattern/query searched in the trie.

Since the child nodes have an ordering, an in-order traversal prints the stored keys in order.

A trie can be seen as a DFA (Deterministic finite automaton) without loops.  A trie can be compressed into an DAFSA (deterministic acyclic finite state automaton).  A trie eliminates prefix redundancy.  A DAFSA additionally also removes suffix redundancy.

Applications: See <<suffix_tree>>

Trivia: The name trie is a pun on re__trie__val.  Originally pronounced it as `tree'.  However, other authors pronounce it as `try', in an attempt to distuinguish it from `tree'.


[[compressed_trie]]
=== Compressed trie (aka radix tree, radix trie, compact prefix tree)
Based on <<trie>>, see there for terms and symbols. Is an implementation of the ordered <<associative_array>> ADT.  A radix tree is a space-optimized trie, where a node with only one child is removed, merging its two adjacent edges into one, which is then labeled with the concatenation of the labels of the two previous edges.  That is each edge is no longer labeled with a single character but potentially with a string. The [[letter_depth]]_letter depth_ of a node is the lenght of the key/path that leads to it.

Each node has at least two children, so there are less internal nodes than leaves. Recall there are as many leaves as stored strings, i.e. k. Thus there are O(k) nodes in the compressed trie.

Data structure used to store a node (having up to Σ childs), and the resulting querry time and space needed, both in the trie (as opposed to the node).

Compared to a hash table:

- A trie can have (depending on how nodes are represented) predictable look-up time +O(k)+.  A hash table has +O(k+n)+ time complexity worst case:  O(k) is used to generate the key, looking up the key is O(1) average but O(n) worst case.
- A trie does not need a hash function
- A trie can provide an ordering of the entries by key.  I.e. a trie supports ordered traversal.
- Locality is worse for a key, since it randomly accesses the nodes.
- A trie typically uses more space than a hash table, since the graph uses quite a lot pointers.

Compared to a binary tree:

- Binary tree has +O(k * (lg n))+ time complexity for look-up, insertion, deletion.  Mind that comparing a key requires +O(k)+; in many times the worst-case occurs, due to long prefixes towards the leaves.

References:

- https://courses.csail.mit.edu/6.851/spring12/scribe/lec16.pdf


[[suffix_tree]]
=== Suffix tree (aka PAT tree, suffix trie)
Based on compressed trie, see there for terms and symbols. Given a text T, append $ (see <<trie>>), then store all suffixes of T in a (compressed) <<trie>>. The value associated with a leaf is the starting position of the suffix in T.

When given multiple documents T^i^, i=1, 2, ..., n, append $~1~, $~2~, ... and $~n~ to each T^i^ respectively, and then throw all suffixes of each document into the suffix tree.

The suffix tree contains O(T) nodes, thuse O(T) space needed (using reasonable representations of the trie, see there).

Details: Seeing a letter as a string of size one, all edges in a compressed trie have strings on them. Such a string can be stored in O(1) by storing only the indicies (within T) of its first and last letter.

Applications:

- <<string_searching>>:
  * Find (all) occcurences of pattern P in text T. Create suffix tree for T. Using P as key delivers subtree whose leaves corresponds to all occurences of P in T.
  * Find first i occurences of pattern P in text T: Augment the leave nodes so they build a linked list, and augment internal nodes by a pointer to its leftmost descendant leaf.
  * Find number of occurences of pattern P in T: Augment internal nodes in the suffix tree with the number of leaves. Use P as key, the found node thus delivers number of leaves, which equals number of occurences.
  * Find longest substring that occures twice in P: Augment internal nodes with their <<letter_depth>>. Then search the internal node with the largest letter depth in O(T) time.
  * Find longest substring that occures in multiple documents T^i^: Similar to above, but look for the internal nodes with maximum letter depth with greater than one distinct $~x~ below.
  * Longest common prefix of two substrings in T in O(1) time: Take the two leaves corresponding to the indexes where the substring start, <<LCA_problem,find LCA>> in O(1) time, and the letter depth of the found node is the answer.
  * Find all occurences of T[i:j]: Instead of, as in a normal search, finding the node from the root in O(j-i) time, find the (j-i)the ancestor of leaf i in O(1) time (via an _LA_query_)


=== Suffix array
*to-do*

References:
- https://courses.csail.mit.edu/6.851/spring12/scribe/lec16.pdf


=== Suffix tray

Is a combination of a suffix tree and a suffix array,

Note that there's this similar idea ....


*to-do*

References:
- https://courses.csail.mit.edu/6.851/spring07/scribe/lec09.pdf


=== DAFSA as data structure
Represents a finite (since it has no cycles) set of strings aka keys.  Single source vertex.  Each edge is labeled by a letter / symbol.  Each vertex has at most one vertex which is labeled with a given letter.  The accepted strings are formed by the letters on paths from the source to any sink / NIL vertex.

Can be seen as an compact form of a trie.  Uses less space than a trie.  A trie eliminates prefix redundancy.  A DAFSA additionally also removes suffix redundancy.  A trie can store attributes for each string aka key, whereas a DAFSA cannot.

Is an implementation of the ADT <<associative_array>>.


[[disjoint_set_linked_list]]
=== Disjoint-set linked list
A data structure implementing the <<disjoint_set>> ADT, using a linked list for each set. The element at the head of each list is chosen as its representative.

*to-do*


[[disjoint_set_forest]]
=== Disjoint-set forest
A data structure implementing the <<disjoint_set>> ADT using a forest. Each element gets two attributes: A reference to parent element/vertex and its rank (see the weighted tree optimization). Imagine the elements to be organized as a forest. Each tree represents a set. find-set(v) means now finding the root of the tree v is in, which is done by chasing parent pointers. merge-sets(u,v) is implemented by attaching the root of the tree of u to the tree of v, or vice verca.

Optimizations: To avoid high trees which would make find-set (i.e. chasing lots of parent pointers up to the root) expensive, two optimizations are made: 1) union by rank: merge-sets always attaches the smaller to the larger tree. In that case the resulting tree only gets higher (by one) if both original trees had the same height. For that, we need to maintain the height of each tree. Due to path compression, the next optimization, we use an approximation of the height which we call rank. 2) Path compression: find-set(v) shortens the path from v to its root by, after finding the root, making the root the parent of v. That might change the height of the tree, but not our rank.

--------------------------------------------------
insert(v):
  parent[v] = v
  rank[v] = 0

find-set(v):
 if parent[v]!=v:
   parent[v] = find-set(parent[v]) // "parent[v] =" is path compression
 return parent[v]

merge-sets(u,v):
  rootOfU = find-set(u)
  rootOfV = find-set(v)

  // naive variant
    parent[rootOfU] = rootOfV

  // union by rank optimization
    if rank[rootOfU]<=rank[rootOfV]: parent[rootOfU] = rootOfV
    else                           : parent[rootOfV] = rootOfU
    if rank[rootOfU]==rank[rootOfV]: rank[rootOfV] += 1
--------------------------------------------------


== Augmenting data structures

1. Choose an underlying data structure DS.
2. Determine additional information AI to maintain in DS.
3. Verify that we can maintain AI for the basic operations on DS.
4. Develop new operations.

Let +f+ be an attribute that augments a red-black tree +T+, and suppose that the value +x.f+ for each node +x+ only depends on only the information in the nodes +x+, +x.left+ and +x.right+. Then we can maintain +f+ in all nodes of +T+ during insertion and deletion without affecting the +O(lg n)+ performance of these operations.


== Misc. related computer science


=== NFA
*to-do*:


=== DFA
*to-do*:


=== DAFSA
*to-do*:


=== Bit manipulation
A _nibble_ is a four bit aggregation (aka _halb-byte_ or quartet).

--------------------------------------------------
set:    x |=  y
clear:  x &= ~y
toggle: x ^=  y
test:   x &   y
--------------------------------------------------

In C / C&plus;&plus;, +CHAR_BIT+ is the number of bits in a byte.

- http://graphics.stanford.edu/~seander/bithacks.html


=== Misc terms
Sentinel:: A sentinel is an object to represent the end of a data structure.


== Problems

=== Overview


--------------------------------------------------
Knapsack
  unbounded knapsack problem (UKP) | DP: time O(n*S) | approx: greedy algo: O(n)
    rod cutting problem: is the same problem
  bounded knapsack problem (BKP) | can be reduced to 0-1 knapsack
  0-1 knapsack problem | DP: time O(n*S)
  continuous/fractional knapsack problem | greedy algo: time: O(n*lg(n))
  coin change problem | greedy (optimal only for canonical coin systems): O(n*lg(n))

Longest common subsequence
TSP. Travelling purchaser problem
edit distance
longest path
shortest path
minimum spanning tree. directed and undirected version
cycle dedection
topo sort
sort
cutting stock problem
bin packing problem
assignment problem
bipartite matching

--------------------------------------------------


[[TSP]]
=== Travelling salesman problem (TSP)
In an weighted graph (directed or undirected), the TSP is finding the path with mimimum weight visiting each vertex exactly once and start vertex being the end vertex.

TSP is a special case of the travelling purchaser problem.  

TSP is NP-complete.

Algorithms:

- Exact: Held–Karp, a dynamic programming algorithm.
- Exact: Various branch-and-bound algorithms
- Exact: ... linear programming ...
- Approximations: *to-do*

Applications: *to-do*


=== Travelling purchaser problem
*to-do*


[[MST_problem]]
=== Minimum spanning tree problem
The problem of finding the <<MST>>, actually minimum spanning forest, in an undirected weighted possibly disconnected graph.

Algorithms:
- <<Kruskal>>'s algorithm
- <<Prim>>'s algorithm (restricted to connected graphs)
- *to-do* more


[[edit_distance]]
=== Edit distance
Given two strings x and y, the edit distance is the minimum cost series of
edit operations that transform x into y.  There are cost tables:
cost-deletete[c] is cost to delete char c from x, cost-insert[c] is cost to
insert char c into x, cost-replace[c1, c2] is cost to replace char c1 by
c2. Doing nothing modeled by cost-replace[c,c].

<<dynamic_programming>>:

1. suproblems: All possible suffixes of x and y.  I.e. edit distance on x[i:]
   and y[i:] for all i∊[0,|x|) and j∊[0,|y|).

2. guess: In each step, there are three choices: ① replace x[i] by y[j] (do
   nothing is modeled by replacing c by c) or ② insert (prepend) y[j] to x or
   ③ delete x[i].  The general idea is to consume the first character of x
   and/or y in order to 1) make first char of x and y equal and to 2) be left
   with a subproblem (to make progress at least one char needs to be
   consumed).

3. recurrence: 
+
--------------------------------------------------------------------------------
DP(i,j) =
  if i=|x| and j=|y|: ④ 0
  else: min(
  ① cost-replace[x[i],y[j]] + DP(i+1, j+1)  if i+1≤|x| and j+1≤|y|,
  ② cost-insert[y[j]]       + DP(i  , j+1)  if             j+1≤|y|,
  ③ cost-delete[x[i]]       + DP(i+1, j  )  if i+1≤|x|            )
--------------------------------------------------------------------------------
+
④ is the base case (aka smallest subproblem), which is the edit distance to
   transform the empty string to the empty string, which obviously is 0.

4. Description of subproblem DAG: Imagine a matrix, each cell represents a
   vertex in the DAG and thus also represents DP(i,j). It has |x| rows indexed
   by i, and |y| columns, indexed by j. Thus the top left cell/vertex is the
   original problem (edit distance to transform x into y), and bottom-right
   cell/vertex is the base case ④.  The weight of the edges are the respective
   cost-x[…] term in the DP formula of step 3.  Optionally each cell/vertex
   can have a value attribute which then is DP(i,j).
+
Example: x=FLO and y=FOO:
+
-----
             outgoing edges of each matrix-cell / DAG-vertex
   FOOε      the cells in the left-most column and bottom-most row
   0123 j    naturally don't have edges leaving the matrix
F 0R···      ☐→① insert
L 1····      ↓ ↘③ replace
O 2····      ② delete  
ε 3···④      
  i          R means root of the DAG, i.e. the original problem
-----
+
bottom-up approach: Solve the subproblems by starting in the bottom
right corner and then going left and/or up.  E.g: ++for i=|x|⋯0: for
j=|y|⋯0: …++.
+
time complexity: #subprobs=Θ(|x|⋅|y|) (number of cells). time/subproblem = Θ(1). Overall running time =
  #subprobs⋅time/subproblem = Θ(|x|⋅|y|).
+
Space complexity: Θ(|x|⋅|y|) (number of cells) for a trivial implementation.
If only a sliding window of one row or column, which ever of |x| or |y| is
smaller, is kept, the space complexity becomes Θ(min(|x|,|y|)).

5. The original problem is DP(0,0).


*to-do*:

- Most sources on the net seem to solve it in terms of making the subproblems
  prefixes, opposed to suffixes as above.  So my matrix above doesn't match
  moste of the pictures / drawings found on the net.
- Backtracing / make the operations needed available to the caller

Applications:

- computational biology: quantify similarity of DNA sequences
- correction of spelling mistakes, i.e. which correct word is the most likely


=== Longest common subsequence (LCS)
Given a set of sequences, typically two, what is (are) the longest common subsequence(s) -- The solution might not be unique, i.e. multiple subsequences of same lenght will qualify as having the longest lenght.  Note that unlike substrings, subsequences are not required to occupy consecutive positions.

In general: NP-hard.

For two sequences: Equals the <<edit_distance>> problem, with cost of insert and delete being 1 and replace being 0 for c→cʹ and ∞ otherwhise.

Applications:
- file comparison, e.g. the diff utility
- bio informatics: as a measure how similar DNA sequences are (the longer the LCS the more similar),


[[knapsack]]
=== Knapsack

0-1 knapsack problem:: Given a set of n items, each item i with a weight w[i]
(an integer) and a value v[i], determine the items to include in a collection
so that the total weight is less than or equal to a given limit S (an integer)
and the total value is maximal.

bounded knapsack problem (BKP):: Removes the restriction that there is only
one of each item, but restricts the number of copies of each item i to c[i].

unbounded knapsack problem (UKP):: Places no upper bound on the number of
copies of each kind of item.

change-making problem:: how can a given amount of money be made with the least
number of coins of given denominations. Similar to UKP, however capacity of
knapsack has to be hit exactly. `weight of item' corresponds to `value of
coin', and `value of item' is always -1.

rod cutting problem:: same as UKP. rod length -> knapsack capacity, length i
-> item i having a weight of i, value of length i -> value of item i.

fractional/continuous knapsack problem:: Instead of items we think of
materials.  There is an certain amount (weight) of each material, and we can
pack any amount less than that per material into the knapsack. Solution: sort
materials descendinding by value/weight, then greedely take of each material
as much as possible until the knapsack is full. O(n*lg(n)).



Solution for the 0-1 knapsack problem using <<dynamic_programming>>:

Put the items in some sequence.

1. Suproblems: All possible suffixes of the item sequence (items[i:]) × all possible remaining capacities X≤S.

2. Guessing: In each step, there are two choices: ① shall I include item i (aka current/front item) or ② shall I not?  

3. Recurence:
+
--------------------------------------------------
DP(i,X) =
  if i=n: ③ 0
  else: max(
  ①        DP(i+1, X)       if i+1≤n            ,
  ② v[i] + DP(i+1, X-w[i])  if i+1≤n and w[i]≤X )
------------------------------------------------------------
+
③ is the base case, which is the knapsack problem for an empty set of items
and whatever remaining capacity: the maximal value is obviously 0.

4. Description of the subproblem DAG: Imagine a matrix, each cell represinting
   a vertex in the DAG and thus also represents DP(i,X).  It has n+1 columns
   indexed by i, and S+1 rows, indexed by X.  Thus the top left cell/vertex is
   the original problem (knapsack for all items and capacity S).  The right
   column are the base cases.
+
Example: n=3 items, capacity S=4:
+
----------------------------------------------------------------------
c   items
a   0124 i   outgoing edges of each matrix-cell / DAG-vertex:
p  4R··③     ☐→① Don't include item i. Edge-weight 0.
a  3···③      ↘② Include item i, which removes w[i] from capacity X.
c  2···③         Edge-weight -v[i].
i  1···③
t  0···③
y  X
----------------------------------------------------------------------
+
bottom-up approach: Solve subproblems by starting in the bottom right corner
and then going left and/or up: E.g.: ++for i=n⋯0: for X=0⋯S: …++.  Space
complexity can be improved by only using a sliding window of two columns.
Note that the top-down approach doesn't need to calculate all n*S vertices; it
only calculates the ones reachable from DP(0,S).
+
time complexity: Θ(1). Overall running time: +Θ(n*S)+,
i.e. <<pseudo_polynomial>>. It's exponential, since +Θ(n*S)+ is exponential
relative to the input size which is +O(n*lg S)+ (think how many bits you need
to represent the input).

5. The original problem is DP(0,S).

6. The items to be included into the knapsack are (for non-zero weights).
--------------------------------------------------
X=S
for i in [0,n)
  if DP(i,X) = v[i] + DP(i-1, X-w[i]): // i.e. if choice ② was made
    // the element i is in the knapsack
    X -= w[i]
--------------------------------------------------

Applications:

*to-do*
*to-do*: process https://en.wikipedia.org/wiki/List_of_knapsack_problems


[[shortest_path_problem]]
=== shortest path problem
The problem of finding a <<shortest_ path>> (there might be multiple shortest paths) in a graph. Recall that the shortest path is undefined, or it's weight is -INFINIT, if it contains negative-weight cycles, because one can always make an allegedly `shortest path' shorter by walking through the cycle one more time.

optimal substructure: The shortest path problem has optimal substructure. A subpath of a shortest path is itself a shortest path; if it wasn't, we could replace it by the alegedly shorter path and thus make the overall path shorter.

Applications: *to-do*


==== single source shortest path problem
general graph: <<Bellman_Ford>> +O(V*E)+

non-negative weights: <<Dijkstra>> +O(E+V*lg(V))+

DAG: <<toposort>>, then for each node, for each neighbor, relax. +Θ(E + V)+

unweighted graph: <<BFS>> +O(E+V)+

integer weights: Thorup *to-do*


==== single destination shortest path problem
Can be reduced to single source shortest path by reversing direction of each edge.


==== single pair shortest path problem
All known algorithms for this problem have the same worst case asymptotic running time as the best single source algorithms.

non-negative weights: <<A_star>>


==== all pairs shortest path problem
general graph (neg weights, dedects neg. cycles): <<floyd_warshall>> in O(|V|^3^)


=== Longest path problem
*to-do*


=== Graph coloring
*to-do*
Relation to four color problem?


[[toposort]]
=== Topological sort
A _topological sort_ (aka _topsort_, _toposort_, _topological ordering_) of a directed acyclic graph (DAG) is a linear ordering of its vertices such that for every directed edge (u,v) from vertex u to vertex v, u comes before v in the ordering.  Toposort is possible only for DAGs; see <<cycle_dedection>> for how to check.

Algorithm: Augment the <<DFS_all_source,DFS-all-source>> algorithm: when an vertex v is finished, insert it onto the front of the output sequence.  Time complexity +Θ(V+E)+.


[[cycle_detection]]
=== Cycle detection

Algorithms:
- <<DFS>> finds a <<DFS_edge_classification,back edge>>.
- Rocha–Thatte, a distributed algorithm.

Applications:
- Dedect cycles (i.e. problems) in a dependency graph


=== Eulerian trail / Eulerian cycle
An Euler tour in an undirected graph is a walk that traverses each edge exactly once.  If such as walk exists, the graph is called _traversable_ or _semi-eulerian_.  An Eulerian cycle (aka Eulerian tour) in an undirected graph is a cycle that uses each edge exactly once.  If such a cycle exists, the graph is called Eulerian or unicursal.

Hierholzer's algorithm solves the Eulerian cycle problem in linear time: +O(E)+.

Trivia: Was first discussed by Leonhard Euler while solving the famous _Seven Bridges of Königsberg_ problem in 1736.


=== Hamiltonian path / cycle
A Hamiltonian path is a path in an undirected or directed graph that visits each vertex exactly once.  A Hamiltonian cycle (or Hamiltonian circuit) is a Hamiltonian path that is a cycle.  A Hamiltonian cycle is a special case of the traveling salesman problem: adjacent cities have distance one, the others distance two, and verifying that the total distance traveled is equal to n.

Determining whether such a path or cycle exists is NP-complete.


[[LCA_problem]]
=== Lowest common ancestor (LCA) problem
See <<DAG>> for the definition of LCA.

Solutions:
- With O(n^2^) space, O(1) time is trivial: An O(1) time lookup table stores the answer for all input pair vertices.
- Tarjan's off-line LCA: After preprocessing the DAG, solvable in O(1) time and O(n) space.
- If DAG is a cartesian tree: reduce to array, see <<cartesian_tree>>, and solve <<RMQ>> there. *to-do*: how exactly, there we're rediricted to LCA


[[LA_query]]
=== Level ancestor problem / query
Given a rooted tree and a node v, LA(v,d) delivers the ancestor node of v which is at depth d, or equalently, delivers the kth ancestor (where parent is the 1st ancestor), k=height(node)-d.

When n is the number of nodes, after preprocessing which takes O(n) time and O(n) space, solvable in O(1) time.


=== Range/interval search

Given a set of values/points (k-Dim), find the ones being in a (k-Dim) range.

Given a set of ranges/geomteric objects (k-Dim), find the ones overlapping/intersection.

See also <<rmq>>

*to-do*

[[RMQ]]
=== Range minimim query (RMQ)

Given an array A, find index (here named k) of minimum element in range [i,j].

++k=RMQ(i,j)=(arg)min{A[n]|i≤n≤j}++

(arg)min says that we're interested in the index, not the value of the minimum element.

Solutions:

- Convert A to a <<cartesian_tree>>, see there, and in that tree, solve the <<LCA_problem>>, see there.


== References
- http://ocw.mit.edu/courses/civil-and-environmental-engineering/1-204-computer-algorithms-in-systems-engineering-spring-2010/lecture-notes/
- https://www.quora.com/What-are-the-very-basic-algorithms-that-every-Computer-Science-student-must-be-aware-of
- MIT 6.006: Introduction to Algorithms: https://courses.csail.mit.edu/6.006
  * (fall'11): https://courses.csail.mit.edu/6.006/fall11/notes.shtml, https://www.youtube.com/playlist?list=PLUl4u3cNGP61Oq3tWYp6V_F-5jb5L2iHb
  * Newest lecture notes which are freely available spring'14: the newest vidoes seem to be from fall'11.
- MIT 6.046J / 18.410J: In '15 "Design and Analysis of Algorithms", before "Introduction to Algorithms (SMA 5503)". Aparantly in '15 it's an advanced course between 6.006 and 6.851, before it was a course similar to 6.006.
 * Spring '15: https://www.youtube.com/playlist?list=PLUl4u3cNGP6317WaSNfmCvGym2ucw3oGp
 * Fall '05:
  ** youtube: https://www.youtube.com/playlist?list=PL81A705FB7F988E7C
  ** OCW, less videos, but title describing content: http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-046j-introduction-to-algorithms-sma-5503-fall-2005/video-lectures/
- MIT 6.851: Advanced Data Structures: https://courses.csail.mit.edu/6.851/
  * spring'12: https://courses.csail.mit.edu/6.851/spring12/lectures/, https://www.youtube.com/playlist?list=PLUl4u3cNGP61hsJNdULdudlRL493b-XZf
  * Newest spring'14: The link to the vidoes are the same as those of spring'12.
- https://www.cs.usfca.edu/~galles/visualization/

// Local Variables:
// eval: (visual-line-mode 1)
// eval: (auto-fill-mode -1)
// eval: (filladapt-mode -1)
// compile-command: "asciidoc -a toc -a icons algorithms_and_data_structures.txt"
// End:

//  LocalWords:  pre th ADT Multimap multihash multimap emptyCount fullCount
//  LocalWords:  useQueue putItemIntoQueue getItemFromQueue Treap DAFSA Deque
//  LocalWords:  BST spw spwfs decreaseKey spt dest unicursal eulerian NPC
//  LocalWords:  Königsberg Hierholzer's subgraph supergraph Horner Horner's
//  LocalWords:  adaptors acc umulator Quickselect supremum infimum CLRS DFS
//  LocalWords:  AStarMonotonicH AStar toposort topsort BSF ith preorder args
//  LocalWords:  inorder Sedgewick Karp textlen patternlen patternhash str
//  LocalWords:  texthash issubstring rollinghash len BFS MyStack prev Thorup
//  LocalWords:  Brodal mergeable
