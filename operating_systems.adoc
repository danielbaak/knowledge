m:encoding: UTF-8
// The markup language of this document is AsciiDoc

== Introduction
=== System calls
System calls are typically wrapped by an library

. Regular foo library method is called in a regular manner.  Push arguments on stack, then execute procedure call instruction.
. The library implementation of foo typically puts the system-call number in a place where the OS expects it, such as an register.  Then it executes a TRAP instruction.  That switches from user mode to kernel mode and start execution at a fixed address within the kernel.
. The kernel code that is executed examines the system call number and then dispatches to the correct system-call handler, usually via an table [system-call number => system-call handler address]. The system call handler runs.  Once it has completed its work, control may be returned to the user-space library procedure with the instruction after the TRAP, or it defers that lets the scheduler schedule another task.
. Once the OS returns control to the instruction after the TRAP, everything continues as normal.

The library procedure, possibly written in assembly language, typically puts the system-call number in a place where the OS expects it, such as a register.

== Processes and threads

A _(computer) program_ is a passive collection of (machine) instructions.  Several processes may be associated with the same program, e.g. when the same program is `opened'/started multiple times.

A _process_ is an instance of an executing program and a way to group related resources such as an address space, file descriptors etc.

A _thread of execution_ (aka _thread_) is the smallest sequence of programmed instructions that can be managed independently by a scheduler.

A _task_ is a unit of execution; in some OS a task is synonymous with a process, in others with a thread.

_Batch processing_ is the execution of a series of programs (_jobs_) on a computer without manual intervention.


=== Processes
A _process_ is an instance of an executing program and a way to group related resources such as an address space, file descriptors etc.  It owns resources such as memory, file descriptors (Unix terminology) or handles (Windows terminology), see PCB below.

_foreground process_ interacts with user, _background process_ does not.

In some systems, a process can have zero or more child processes and one parent process, thus forming a tree from a graph point of view, and also forming a _process hierarchy_. A subtree is called a _process group_. In UNIX, +_init_+ is the root process; it is represented in the boot image.

_process states_: _running_: currently using the CPU, _ready_: runnable but temporarily stopped, _blocked_: unable to run until some external event happens.

In a strictly technical sense, a Unix-like system process is a _daemon_ when its parent process terminates and the daemon is assigned the init process (process number 1) as its parent process and has no controlling terminal.  However, more commonly, a daemon may be any background process, whether a child of the init process or not.

_process-table_ (aka _process control block_ (_PCB_)) is a data structure owned by the OS containing the information needed to manage a particular process.  The PCB is "the manifestation of a process in an OS".  Process management: Registers (incl. PC, SP), process state/priority/ID, scheduling parameters, parent process, signals, ... Memory management: Pointer to text/data/stack segment info.  File management: Root directory, working directory, file descriptors, user ID, group ID.

Depicted differently: _Per-process items_: address space, open files, child processes, pending alarms, signals and signal handlers, accounting information.  _Per-thread_ program counter, registers, stack, state


=== Threads
A _thread of execution_, shortened to _thread_, aka _lightweight process_, is the smallest sequence of programmed instructions that can be managed independently by a scheduler.  The _thread control block_ (_TCB_) is a data structure owned by the OS which stores the thread specific information needed to manage it: program counter, stack pointer, state of thread, registers, pointer to PCB.  The threads within a process share some of the process' resources such as a common address space, file descriptors, signals.

Threads are more lightweight and faster to create (10-100 times on many systems) and destroy compared to a process.  Thread switching is also much more efficient than process switching, see also <<scheduling>>.  Motivation of threads: Thread B can still accomplish work while thread A is blocked, so their parent process overall still gets work done.

Thread primitives: __`thread create'__: obvious.  __`thread exit'__: terminates thread.  __`thread_yield'__: thread voluntarily gives up CPU.  __`thread join'__: calling thread blocks until a given other thread exits.

_user-level threads_: Threads implemented in user space.  The kernel is unaware of them.  Threads are implemented by a library aka the run time.  Thread primitives (create etc.) can be handled there instead by a system call, which is much faster.  Scheduling is hard tough, since the run time only gets a chance to do so when explicitly called in some way by the currently running thread.  Also when one thread blocks, no other thread can continue, which defeats the motivation of having threads.  Historically many system calls are blocking, and one of the motivations of implementing threads in the user space is not having to change system calls, so it makes no sense to change them to non-blocking.  Also, if a thread triggers a page fault, no other thread can continue.

_kernel-level threads_ (aka _native threads_): Threads implemented in the kernel.  Deducible from the above about user-level threads: Thread primitives are now much more expensive since they are system calls now.  We gain that when a thread that blocks (due to any system call or page fault),  other threads (possibly of another process) can continue.


[[scheduling]]
=== Scheduling
Scheduler's concerns:

- _Throughput_: Total number of processes that complete their execution per time unit.
- _Latency_ :
 * _Turnaround time_: Total time between submission and *to-do* ??????
 * _Response time_: Time between a thread's request until the first response.
- _Fairness_: Equal CPU to each thread, that is according to each process' priority and workload
- _Waiting time_: The time the process remains in the waiting queue.
- _deadlines_: In an RTOS, ensure that threads can meet deadlines.

OS may feature up to three distinct types of scheduler:

1) A _long-term scheduler_ (aka _admission scheduler_) decides which requests to execute a program (and thus put it in main memory) are granted and which are delayed.  Selects a good mix of I/O-bound processes and CPU-bound processes, so non of the two resources CPU or I/O is left seldom used.

2) A _medium-term scheduler_ temporarily removes processes from main memory and places them  on secondary memory (such as a disk drive) or vice versa.  This is commonly refereed to as _swapping out/in_ (also *incorrectly* as `paging out/in').

3)  _short-term scheduling_ (aka _CPU scheduler_) decides which of the ready threads are to be executed (allocated a CPU).  A _dispatcher_ is the module that does the context switch, switches to user mode, jumps to the proper location in the user thread to restart it by its new state.

Scheduling disciplines:

- _First in first out_ (aka _First come, first served_ (_FCFS_)):

- _Fixed priority preemptive scheduling_. Each process has a fixed priority.  Priority queue.  Incoming higher priority thread interrupts running lower priority thread.

- _Round-robin_.  Each thread gets assigned a fixed time unit.  The scheduler cycles through the threads in the ready queue.

- _multilevel queue scheduling_. *to-do*


==== Context switch
In computing, a _context switch_ is the process of storing and restoring the state (context) of a process or thread so that execution can be resumed from the same point at a later time.

There are three potential triggers for a context switch:

- _multitasking_:
 * This context switch can be triggered by the threads making itself unrunnable, such as by waiting for an I/O or synchronization operation to complete.
 * On a _cooperative_ multitasking system the threads must be explicitly programmed to yield when they do not need the CPU anymore.
 * On a _preemptive_ multitasking system, the scheduler may also switch out processes which are still runnable.  The period of time for which a process is allowed to run in a preemptive multitasking system is generally called the _time slice_, or _quantum_.  The OS gets in control to arrange for a context switch via an interrupt.

- _interrupt handling_:  A programmable interval timer is required which invokes an hardware interrupt (*to-do* <- correct wording?)

- _user and kernel mode switching_: When a transition between user mode and kernel mode is required in an operating system, a context switch is not necessary; a mode transition is not by itself a context switch.  However, depending on the operating system, a context switch may also take place at this time.

_Thread switching cost_ (it is implied within the same process) is dominated by the cost of switching out the registers and entering and exiting the kernel.  On systems such as   Linux, that is very cheap.

_Process switching cost_ is far more expensive.  Most cache lines become useless, the TLB gets flushed, thus memory access will be much more expensive for a while.



== Interrupts
Devices notify the interrupt controller.  The interrupt controller puts a number on the address lines and asserts a signal to interrupt the CPU.  The CPU interrupts its current work, saves (see later below) certain information such as the program counter to be able to continue the interrupted work, and then uses the number on the address lines used as an index into a table called the _interrupt vector_ which translates such numbers to an address of an interrupt-service procedure.  Often that interrupt is shared between interrupts and traps.  The interrupt-service routine acknowledges the interrupt by writing a certain value to one of the interrupt controller's I/O ports.  There is not necessarily a context switch, either before calling the interrupt-service or within the interrupt-service routine.

_HW interrupts_ are used by devices to communicate that they require attention from the OS.  The act of initiating a HW interrupt is referred to as an _interrupt request_ (_IRQ_). _SW interrupt_.  A _SW interrupt_ is caused either by an exceptional condition in the CPU itself (e.g. division by zero), or a special instruction.

*to-do*:
- In which context is an interrupt service routine executed?  What are
 the impacts regarding topics such as re-entrancy and thread-safety.
- signal handlers


== Interprocess communication

[[mutual_exclusion]]
=== Mutual exclusion / Synchronization meaning `not at the same time'
_(problematic|benign) race condition_: The outcome is dependent on the timing of processes/threads, e.g. the exact point in time when they are interrupted or when they continue. Strictly speaking the term race condition encompasses also benign outcomes like multiple threads adding to the same queue. However commonly the term is used to mean problematic outcomes, e.g. when invariants are broken. Race conditions typically occur when an shared resource is not protected by a mutual exclusion primitive. _critical region_ (aka _critical section_) is the part (which might consist of discontiguous fragments) of a program that accesses a shared resource. _Mutual exclusion_ is some way of making sure only one thread/process is executing a critical region (aka protect the critical region), which is not the same as <<synchronization>>. A good solution needs four conditions:

- No two processes may be simultaneously inside their critical regions.
- No assumptions may be made about speeds or the number of CPUs.
- No process running outside its critical region may block any process.  (so the process inside the critical region is never prevented from leaving it)
- No process should have to wait forever to enter its critical region.

_busy waiting_: continuously testing a variable until some value appears.

*to-do*: what exactly must be protected, i.e. what exactly is meant with a shared ressource in this context? Probably 1) multiple data (what is a single datum in this context? see next points) constituting an invariant, and we have to ensure the invariant holds 2) Is read/write to a single char/byte also something that must be protected, from a portable point of view? 3) What about native type (int) of a cpu? What about larger types, such as double, long long, long double?

*to-do*: term data race


=== Locks
A _lock_ enforces mutual exclusion.  Lock is the name of a concept.  Semaphore or mutex are well known examples of specific implementations.

Lock concepts:

- _lock overhead_: The extra resources for using locks, like memory space,  CPU time to initialize, destroy, acquire, release locks.
- _lock contention_: Whenever a thread attempts to acquire a lock held by another process.
- _deadlock_: See chapter <<deadlock>>.
- _readers-writer lock_ (aka _RW lock_, _shared-exclusive lock_, _multi-reader lock_, _push lock_, _multiple readers/single writer lock_): Multiple threads can have shared ownership (typically for reading). But at most one thread can have exclusive ownership (typically for writing), and if any thread does have exclusive ownership, no other threads can have shared or exclusive ownership.



Lock properties:

- _advisory lock_: In general, locks are advisory locks, where each thread cooperates by acquiring the lock before accessing the corresponding data.
- _mandatory lock_: Some systems also implement mandatory locks, where attempting unauthorized access to a locked resource will force an exception in the entity attempting to make the access.
- _granularity_:  A measure of the amount of data the lock is protecting.  _Coarse granularity_ means a small number of locks, each protecting a large segment of data.  That results in less lock overhead but increases lock contention.  _Fine granularity_ means a large number of locks, each protecting a small amount of data.  That increases lock overhead but decreases lock contention.

Lock disadvantages:

- <<deadlock>>
- <<livelock>>
- <<priority_inversion>>
- They cause blocking.  *to-do*: list counter measures
- Lock overhead: I.e. overhead to access a resource, even if the chances for collision are very rare
- Lock contention limits scalability, which is one of the main reasons to use threads
 * Counter meassure: While having lock acquired, only do the minimal operations needed on the shared ressource which absolutely must be done while being protected by a lock.
- Finding the optimal balance between lock overhead and lock contention is difficult
- Difficult to reason about, resulting in programs expensive to maintain and prone to bugs
- _lock convoy_:  When multiple threads wait for the same lock, and the wait is not free of run-time costs.  That includes the case where the lock is released, and the threads not getting the lock have to pay runt-time costs.  Thus CPU time is burned for no real benefit.  Or from the thread point of view, he uses up the remainder of its scheduling quantum for no real benefit.
- An interrupt handler cannot savely aquire a lock since the thread preempted by the interrupt may hold the lock.

Some programming pitfalls / tips:

- Be carefull to not pass out pointers/references to data protected by a lock, unless you're sure the recipent will only access the data via the lock. That includes don't pass pointers/refs to other functions, unless you're sure they don't store them.
- Only have lock acquired for the shortest possible amount of time. I.e. while having lock acquired, only do the absolut neccessairy operations on the shared data. I.e. don't do operations that also could be done while not protected by the lock.  For other operations, give up lock. That might mean lock/unlock a few times (in C++, e.g. using unique_lock). On the flip-side, locking/unlocking also adds some overhead.
- If working with non-recursive mutexes, between locking and unlocking it, best call no other functions, or only functions where you definitely *know* they won't ultimatively lock the same mutex again.
- C&plus;&plus;: Consider choosing once_flag, call_once over mutex
- C&plus;&plus;: Starting with C&plus;&plus;11, the initialization of static local variables is guaranteed to be thread safe.
- When writes are much more seldom than reads, consider using a readers-writer lock. C&plus;&plus;17 provides shared_mutex, shared_lock.
- Be carefull not to delete objects like mutex, condition_variable etc. while any thread is still using them.


=== Lock implementations
_spinlock_: *to-do* elaborate more.  say that probably internally an atomic `test and set' (what is the generic term?) is used.

_atomic `test and set'_ operations: *to-do*: elaborate

_semaphore_:  Is about mutual exclusion.  Quite a general construct.  A semaphore is a kernel object which allows entering n active threads from many others, (*to-do:* among different processes?), to enter the critical region.  The other non selected threads are put to sleep.  No concept of ownership.  (*to-do*: I'm not sure: Anyone can signal, also a thread not currently having acquired access to the critical region).  Metaphorically speaking, a semaphore is a record how many units of an particular resource are available, coupled with operations to safely (without race conditions) modify that record.  _counting semaphores_ (aka _general semaphore_) allow an arbitrary resource count, while _binary semaphores_ are restricted to the values 0 and 1 (or _locked_/_unlocked_, _unavailable_/_available_).  The operations / primitives of counting semaphores are _wait_ (or _P_ (Proberen -- try)) which decrements the semaphore and _signal_ (or _V_ (Verhogen -- raise)) which increments the semaphore.  Optimally the two have an parameter defining the number of units the semaphore is to be decremented/incremented.  Many OS provide efficient semaphore primitives that unblock a waiting process when the semaphore is signaled / incremented.  To avoid starvation, a semaphore has an associated queue of processes.  There are multiple policies how to choose the process that is dequeued next: 1) FIFO 2) Priority queue 3) others.

_Mutex_ (*wrongly* also dubbed _mutex semaphore_):  Is about mutual exclusion.  Can be seen as binary semaphore which additionally has the concept of ownership.  The net effect is that mutexes don't have some of the semaphore's problems, see also below.  A mutex is a kernel object used for allowing only one active thread from many others, among different processes, to enter the critical region.  The other non selected threads are put to sleep.  Supports thread ownership (the thread that locked it), thread termination notification, recursion (multiple `acquire' calls from same thread) and `priority inversion avoidance' (see also below).  May also provide deletion safety, where the thread (*to-do* process?) holding the mutex cannot be accidentally deleted.  Only the owner can signal.

_Monitor_: Two common definitions 1) A construct allowing threads to have both mutual exclusion and the ability to (efficiently) wait for a certain condition to become true. Typically consists of a mutex and a condition variable 2) (aka _thread-safe object/class/module_) A thread safe object/class/module that uses a `mutual exclusion wrapper' around its members.

_Critical Section Object_: A critical section can be used only by the threads of a single process.  The other non selected threads are put to sleep.  There is no way to tell whether a critical section has been abandoned.

_condition variable_: Allows one thread to notify one or more allready waiting threads; the waiting threads being in a queue. Or in other words: a mechanism so an thread can wait efficiently for a condition to become true. You must make sure not to start waiting after the notify already happened. (Or the other way round, you must make sure not to notify when at least one already is waiting.). Also, the wait function has suffers from spurious wake ups.

to-do:
- Is it _in general_ wise to unlock mutex before notifying, in order to prevent the case that a thread is woke up, only to discover that it can't acquire the lock and being sent to sleep again. A so-called "hurry up and wait" situation. Then again, some implementations move the thread from the condition variable's wait queue directly to the mutex's queue, thus not waking it up; it is woken up later by the mutxe's wait.
- If yes, why should the `sender' thread protect writing the flag with a mutex?
           
*to-do*: questions: Which are blocking (which do specify that)? Which allow for multiple readers?  The ones that talk about processes -- are different user-level/kernel-level threads in one process treated as the same entity?  I.e. I still can have troubles if different threads in one process access the critical region.

- The notifying thread should protect setting state variable with the mutex

// thread 1
// ---------
// note that the sequence of checking predicate and then calling wait is under the protection of the mutex
mutex.lock();
// 1) loop instead a simple if due to spurious wakeups
// 2) having the ok variable ensures to not call wait after notify has already been called, which would result in an eternal wait
while(!done) 
  // point 1, se
  // wait:
  // precondition: mutex must be held
  // action: release lock, waits, wakes up
  // potentially spurious, acquires
  // lock
  cv.wait(mutex);
mutex.unlock();


// thread 2
// ---------
mutex.lock()
// must be under the protection of the mutex to prevent the case setting it to
// true while other thread is at point 1, and before we call notify, the other
// resumes at 1, calling wait before we call notify
done = true 
mutex.unlock(); // variant 1. Seems to be prone to subtle bugs. 1) Not a viable variant if the other thread would delete the condition variable (e.g. because it exists) after seeing the precicate to be true, causing the following notify to cause havok. 2) Not a viavle variant if there are more than two involved threads, and the waiting threads, after getting trough the `condition guard', modify the variable.
cv.notify() 
mutex.unlock(); // variant 2. Seems to be the safer variant in general. But might be less optimal (according to cppereference) than variant 1, due to the "hurry up and wait" situation for the other thread (condition variable wakes it up, only to be sent to sleep right away by the locked mutex). On the other hands, some (most?) implementations implement `wait morphing': defer wakeup from condition variable to mutex.

Personal bottom line: Variant 2 is always more robust than Variant 1, and Variant 1 is only an optimization over Variant 1. You can only optimize what you measure. On you're system, the implemention might make use of wait morphing, in which case it gets less likely that Variant 1 is really more performant for your use case / data. http://www.domaigne.com/blog/computing/condvars-signal-with-mutex-locked-or-not/ (see also references there)


Problems of semaphores solved by mutexes:

- _Accidental release_: (accidentaly not calling wait before calling signal).  Each time the buggy code is executed, the semaphore's count is wrongly increased, and one-more-to-many threads can enter the critical region.  With a mutex, thanks to the OS knowing the owner,  accidentaly signaling / releasing a mutex results in an error and the mutex left unchanged.

- Deadlock:

 * _Recursive deadlock_:  A thread tries to lock a semaphore it has already locked.  With a mutex, thanks to the OS knowing the owner,  the owner is allowed to lock multiple times as long as he unlocks the same number of times.  However not all OS provide this feature.

 * _Thread-death deadlock_:  A thread holding a semaphore dies or is terminated;  the other threads will starve.  Mutex: _death detection_: If a thread terminates for any reason, and currently owns a mutex, the OS can initiate cleaning up actions.  Various models exist, two dominate: 1) All tasks are readied with an error condition.  They know an error happened, and must assume the critical region is in undefined state.  No thread is the owner.  The mutex is in an undefined state and must be reinitialized.  2) Only one thread is readied, he gets the ownership, he must assume the critical region to be in undefined state and is responsible for ensuring its integrity.  He can afterwards unlock the mutex as normal.  Not all OS provide this feature.

- _Priority inversion_:  See <<priority_inversion>> for the problem statement.  Mutex: since the owner is known, counter measures explained in <<priority_inversion>> can be applied.

Problems that semaphores, mutexes have -- solved by monitor

- _non-cooperation_:  In the case critical regions are not guarded by mutual exclusion primitives, i.e. a SW bug.  Monitor:  A user space SW solution, where the access to the critical resource is encapsulated, e.g. in a OO class.  That class ensures that the critical region is always properly guarded with mutexes.  This way the region of code being prone to bugs which result in not properly guarding this specific critical region is much smaller.


Problems that both semaphores, mutexes and monitors have:

- _circular deadlock_ (aka _deadly embrace_):  See <<deadlock>>

Solutions to implement a lock

- On single CPU/Core system, temporarily disable preemption and/or interrupts
- Using atomic machine instructions like (e.g. XCHG (exchange, swap), CAS/CMPXCHG (compare-and-(if-equal)-swap), test-and-set (TSL) -- returns old value, fetch-and-add  -- returns old value) and busy waiting via a spin lock.  However, due to the spin lock, that wastes CPU time for a process waiting before a blocked lock.  Also it calls the rather expensive atomic machine instructions (they need to take care of memory bus and caches so its really atomic) many times in a row.  Prone to priority inversion problem, since high priority process is blocked by busy waiting, and the low priority process being in the critical region is blocked by the high prio process.

More info:

- https://blog.feabhas.com/2009/09/mutex-vs-semaphores-%E2%80%93-part-1-semaphores/


[[synchronization]]
=== Synchronization meaning `at the same time'
Some use `task synchronization' to refer to `coordination of tasks', which would include mutual exclusion (make sure things happen not simultaneously).  Others use `task synchronization' in a stricter sense which means `ensure that things happen simultaneously'.

signals

=== Events / Synchronization meaning `not before an event' / Asynchronous communication
Make sure a task does X only after event Y occurred.

OS Primitives which could be used

- RTOS signals: (*to-do*: what is the correct name to distinguish them from UNIX signals?)
- Message queue:  Often very efficiently implemented in RTOSes.
- Shared Memory: (*to-do*: is this a good idea?)

Higher level patterns:

- Callback
- Observer pattern
- `event loop' (loop, wait for events or take them out a queue (_dispatch_)) *to-do*: more precise

*sub-optimal* OS primitives to use

- Semaphores:  Often used for that purpose, but rather error prone since semaphore are meant for another purpose, namely mutual exclusion.
- Interrupts:  (*to-do*: verify its true:) A thread cannot wait for an interrupt to occur
- UNIX signals:  The OS interrupts the running thread and calls a previously registered signal handler.  (*to-do*: verify its true:) Thus a thread cannot wait for an signal to occur.  Also UNIX signal handling is vulnerable to race conditions.
- Mutex: Doesn't work at all, since only the owner can signal / unlock


=== Non-blocking algorithm (superset of lock/wait-free)
In computer science, an algorithm is called _non-blocking_ if failure or suspension of any thread cannot cause failure or suspension of another thread.  Non-blocking guarantees ordered from strongest to weakest: 1) A _wait-free_ algorithm provides the _wait-freedom_ guarantee, which guarantees system-wide throughput and starvation-freedom.  Wait-freedom is the strongest non-blocking guarantee of progress.  2) A _lock-fee_ algorithm provides the _lock-freedom_ guarantee, which allows individual threads to starve but guarantees system-wide throughput, i.e. at least one thread makes progress. All wait-free algorithms are lock-free.  3) An algorithm is _obstruction-free_ if at any point, a single thread executed in isolation (i.e., with all obstructing threads suspended) for a bounded number of steps will complete its operation. All lock-free algorithms are obstruction-free.  Obstruction-freedom demands only that any partially completed operation can be aborted and the changes made rolled back.


[[priority_inversion]]
=== Priority inversion
The problem of that a low priority task can block an higher priority task, in case the high priority task waits for a lock owned by the low priority task.  If the high priority task is left starved of resources, it might lead to a system malfunction.  Priority inversion can also reduce the perceived performance of the system.

Solutions:

Ensure mutual exclusion not by locks but by disabling interrupts / preemption or by non-blocking synchronization algorithm.

The _(Basic) Priority Inheritance Protocol_ enables a low-priority task to inherit a higher-priorities task’s priority if this higher-priority task becomes blocked waiting on a mutex currently owned by the low-priority task.  The low priority task can now run and will eventually unlock the mutex, and at that point it is returned back to its original priority.

_Priority Ceiling Protocol_:  Each resource is assigned a priority ceiling, which is equal to the highest priority of any task that might lock the resource.  Temporarily the priority of tasks is raised in certain situations.


[[deadlock]]
=== Deadlock
A _deadlock_ is a situation in which two or more competing actions are each waiting for the other to finish, and thus neither ever does.

_Coffman conditions_: A deadlock can occur if all the four following simultaneously; Unfulfillment of any of the four is enough to preclude a deadlock from occurring.

1. _Mutual exclusion_: There are at least two unshareable resources. An unshareable resource implies it needs to be protected via mutual exclusion.

2. _Hold and Wait_ or _Resource Holding_: A process is currently holding at least one resource and is requesting one or more additional resources which are being held by other processes.  That implies _multiple processes must acquire more than one shared resource_.

3. _No preemption_: A resource can be released only voluntarily by the process holding it.

4. _Circular Wait_ (aka _deadly embrace_): Thread T1 owns mutex M1 and is waiting for mutex M2, which is owned by thread T2 which is waiting for mutex M1.  This can e.g. happen when each of two task needs two acquire two mutex (e.g. one for accessing input data and one for accessing output data) to enter the critical region.

Note that's not only about waiting for locks (protecting ressources), but also e.g. to wait for another thread to finish (within join).


==== Deadlock handling: Ignoring deadlock
This is used when the time intervals between occurrences of deadlocks are large and the data loss incurred each time is tolerable.


==== Deadlock handling: Deadlock detection and correction
First detect a deadlock (*to-do*: how?), then correct it by:

- _process termination_: One or more involved threads are terminated.

- _resource preemption_: *to-do:* I don't understand this


==== Deadlock handling: Deadlock prevention

_Deadlock prevention_ works by preventing one of the four Coffman conditions from occurring.

- _remove the mutual exclusion_ condition, i.e. remove having at least two unshareable resources.
  * Merge the two ressources into one ressource
  * Spooling (probably seldom applicable *to-do*: explore more)
  * A non-blocking synchronization algorithm.  E.g. _read-copy_update_ (_RCU_).  *to-do*: explore them.

- _remove the hold and wait_ condition: *to-do*: elaborate -- but anyway, the solutions seems to be in-practical

- _remove no preemption_ condition: *to-do*: elaborate -- but anyway, the solutions seems to be in-practical

- _remove the circular wait_ condition.  For example by:
 * Disable interrupts and preemption during critical sections
 * <<dijkstra_solution>>
 * <<arbitrator_solution>>
 * <<lock_hierarchy>>
 * Chandy/Misra solution: *to-do*:

[[dijkstra_solution]]
===== Dijkstra's solution (aka lock ordering)
* Assign a partial order to the resources
* Establish the convention that all resources will be requested in their order.  Order of release does not matter.  That includes that when already owning a resource and wanting to acquire a next, lower order one, that the higher order resource is released first to `start from the beginning'
* No two resources unrelated by order will ever be used by a single unit of work at the same time *to-do* I don't properly understand that yet

Disavdantages: Often the list of required resources in not completely known in advance.  E.g. processes that access large number of database records would not run efficiently if they were required to release all higher-numbered records before accessing a new record, making the method impractical for that purpose.


[[arbitrator_solution]]
===== Arbitrator solution
The set of resources RS={R1,R2,...,Rn} is itself regarded as a resource T which is protected by yet another lock (e.g. mutex) LA (the arbitrator).  Multiple acquire operations on the members of a sub-set of RS as a whole is an operation that has race conditions and thus needs mutual exclusion. Disadvantages:  Looking at the dining philosophers problem:  if the philosopher next to an eating philosopher wants to eat, he waits for the `arbitrator' lock and after acquiring the arbitrator he needs to wait for both forks to become free, which does not happen until is neighbor stops eating. During this waiting the time the arbitrator is locked, thus also those other philosophers can't eat which two forks would be free.


[[lock_hierarchy]]
===== Lock hierarchy
Really only a particular case of Dijektsra's solution


==== Deadlock handling: Deadlock avoidance
*to-do:* Difference to previous sub chapter?

If the OS is given enough information about the future, the OS will only grant requests for resource allocations that lead to a save state of the `resource allocation system'.  The OS must know which resources are currently available, which resources are currently allocated to each process and the resources that will be required/released by each process in the future.

- Bankers algorithm
- Wait/Die
- Wound/Wait


=== Starvation
A thread is in starvation if it waits for a resource which keeps getting given to other threads.  That differs from a deadlock were all participating threads wait for one or more resources.

A possible solution is to use a scheduling algorithm with a priority queue which also uses the aging technique.  Older threads get a higher priority.


[[livelock]]
=== Livelock
A _livelock_ is similar to a deadlock, except that the states of the threads involved in the livelock constantly change with regard to one another, none progressing.  Is a special case of resource starvation, where not only one but all threads involved are not progressing.

A real-world example of livelock occurs when two people meet in a narrow corridor, and each tries to be polite by moving aside to let the other pass, but they end up swaying from side to side without making any progress because they both repeatedly move the same way at the same time.


=== Only seamingly a race condition

Say the reader reads from the criticical section having a boolean value in it,
e.g. `is free'. If not free, it continues to search another free. Say the read
1returns `is _not_ free', then the reader continues to search onther free
element. The case that just after the read, the boolean value might have changed
to `is free' is ignorable, it's just that we missed an opportunity.


== Storage hierarchy

From fast(latency&througput)/expensive/lowcapacity to slow/cheap/highcapacity. Sequenciall access typically is considerably faster than random access (only looking at the medium itself, not considering caching techniques through other layers in the storage hierarchy).

--------------------------------------------------
                  Registers
                  On-Chip Cache
                  On-Board Cache
Primary Storage   RAM
Secondary Storage SD
Tertiary storage  Disk
                  Tape
--------------------------------------------------

== Memory

When wasting memory to save cost of calling free/malloc, e.g. by reusing a
larger than needed region: Memory is wasted, but note that only half a cache
line is wasted, assuming the still used part is of random size.  When having
collections, and each element's size is a multiple of a cache line, then zero
cache is wasted, only memory.


- http://duartes.org/gustavo/blog/post/anatomy-of-a-program-in-memory/
- http://duartes.org/gustavo/blog/post/how-the-kernel-manages-your-memory/
- http://gperftools.googlecode.com/svn/trunk/doc/tcmalloc.html
- linux /proc/meminfo: https://www.centos.org/docs/5/html/5.2/Deployment_Guide/s2-proc-meminfo.html
- http://www.akkadia.org/drepper/tls.pdf, http://docs.oracle.com/cd/E19683-01/817-3677/chapter8-1/index.html, https://en.wikipedia.org/wiki/Thread-local_storage#Pthreads_implementation



// Local Variables:
// eval: (visual-line-mode 1)
// eval: (auto-fill-mode -1)
// eval: (filladapt-mode -1)
// End:

//  LocalWords:  CAS CMPXCHG TSL Verhogen TCB Coffman streit dijkstra XCHG
//  LocalWords:  Vermittler Proberen FCFS IRQ
