Google test is a framework for writing C++ tests. At Flori we use it to write proper [[Unit Test]]s which we call UnitTest2. Differentiate Google test from [[Googlemock]], the later being a C++ framework for creating [[Automated_Test#Test_double|mocks]]. 

An howto write Flori UnitTests2 can be found [[UnitTest2 Practice|here]]. It really doesn't make sense to write an overview / howto for Google test. The official manuals are excellent. Entry point for all manuals is [http://code.google.com/p/googletest/wiki/Documentation here], here's the direct link to the [http://code.google.com/p/googletest/wiki/Primer primer] (aka introduction).

== Ideas ==
The following is a set of ideas how we could improve googletest, or more generally speaking, our test environment. See also [[UnitTest2_Practice#Open_points]] for more general ideas.

=== Oracle ===
When multiple tests fail, the oracle orders the failed tests, so we can look at the most likely culprit first and at the less likely culprit last. It does so using a heuristic:
* It takes into account what changed since the last successful test run (using Git)
* It uses the dependency graph of involved classes (using an approximation: the #include graph which can be generated using the compiler)
* A future ABORT_IF family of macros, which complement the ASSERT & EXPECT family of macros, give hints whether a test failed because the specification wasn't met (a true test failure), or whether there was some problem running the test and whether the specification was met or not is undefined.

=== ABORT_IF ===
Make an explicit family of guard assertions (see xUnit Pattern), e.g. named ABORT_IF. They give hints whether a test failed because the specification wasn't met (a true test failure), or whether there was some problem running the test and whether the specification was met or not is undefined.

If ABORT_IF fails, that means that another test / UUT is the culprit.

In an ideal test world, a test would like to only have the assertion macros in the verify test section. These are the essence of the test: they verify that the UUT meets the specification. How to get there should be magically pop into existence. But in reality, we have to write setup / tear down code. And that code can fail for some unforeseen reasons; we can have NULL pointers although not expecting them. As a consequence, we can't properly run the test. Whether or not the UUT meets the specification or not is left undefined. As defined in [[UnitTest2 Practice]], the only thing tested in a unit test are the calls to the UUT in the exercise section. Everything else must have its test somewhere else. A single test only should 'properly' (non - ABORT_IF) fail if these methods of the UUT have a defect. 

Use case:
* Avoid test errors 
* To test that the setup test section really did what it should. 

See also Oracle, which makes use of the valuable information that ABORT_IF delivers

==== open points ====
ABORT_IF and ASSERT/EXPECT have inverted conditional logic. The first fails if true, the others fail if false. We could make an E(RROR_)ASSERT macro, but that sounds like asserting that there ''is'' an error.

=== AIM / SPEC / EXAMPLE ===
Sometimes it makes sense to put multiple sub-specifications into one test aka specification, or have multiple examples (sub-)specification aka test. The code and the test's output should contain these sub-specifications and/or examples in prose. See also [[UnitTest2_Practice#Test_rules_and_guidelines|Test rules and guidelines]]. Streaming the sub-specification and/or example into the EXPECT / ASSERT has the ''dis''advantage that:
* The (sub-)specification comes after the macro call, but usually we want to see the spec first (e.g. normally we write comments before a block of code, not at the end of it)
* Also in the output we want to have the (sub)spec first, and then the details why the assertion failed.

We could introduce e.g. a AIM macro which is streamable (with >>) 'into' an EXPECT/ASSERT so we could write:

<pre>
"Should reject leading and trailing garbage" >>
EXPECT_FALSE(Convert("x13:00:00 10/08/13x"));

AIM("Should reject leading and trailing garbage") >>
EXPECT_FALSE(Convert("x13:00:00 10/08/13x"));

EString spec = "Should reject a too large values of ";
AIM(spec << "hour")   >> EXPECT_FALSE(Convert("25:00:00 10/08/13"));
AIM(spec << "minute") >> EXPECT_FALSE(Convert("13:60:00 10/08/13"));

spec = "Should reject an %s given with more than two digits";
AIM(spec,"hour")   >> EXPECT_FALSE(Convert("130:00:00 10/08/13"));
AIM(spec,"minute") >> EXPECT_FALSE(Convert("13:000:00 10/08/13"));
</pre>


This principle naturally also can be used for other assertions like ABORT_IF, EXPECT_CALL, EXPECT_THAT etc:
<pre>
// setup
....
"Given is a reference count of one" >>
ABORT_IF_NE( 1, UUT.ReferenceCount() )

// exercise
....
</pre>

=== Test outcomes ===
This is actually only a summary of the above. With everything implemented, possible test outcomes now are:

{| class="wikitable"
|-
! scope="row"|Success
|nothing failed, SUCCEED
|UUT fulfills specification
|-
! scope="row"|Failure
|ASSERT, EXPECT, FAIL
|UUT does not fulfill specification
|-
! scope="row"|Aborted
|ABORT_IF
|It is undefined whether UUT fulfills specification.
|-
! scope="row"|Error
|
|It is undefined whether UUT fulfills specification.
|}

The term 'error' as outcome of a test is described in book xUnit Patterns.

== See Also ==
* [[Testing]] Parent level page with all the testing links, continue from there
